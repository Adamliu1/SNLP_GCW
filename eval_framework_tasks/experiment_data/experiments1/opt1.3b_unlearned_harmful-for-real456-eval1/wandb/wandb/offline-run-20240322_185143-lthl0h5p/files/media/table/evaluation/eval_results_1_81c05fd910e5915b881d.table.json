{"columns": ["Tasks", "Version", "Filter", "num_fewshot", "Metric", "Value", "Stderr"], "data": [["winogrande", 1.0, "none", null, "acc", "0.5840568271507498", "0.0139"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_max", "15.246348660498525", "0.5920"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_acc", "0.3525091799265606", "0.0167"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_diff", "-3.105826935305811", "0.5838"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_max", "32.63387486462991", "0.9235"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_acc", "0.24357405140758873", "0.0150"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_diff", "-7.615203389960682", "0.7979"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_max", "17.3055519168048", "0.8627"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_acc", "0.14443084455324356", "0.0123"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_diff", "-7.1622619265499745", "0.8031"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_max", "30.024489229251536", "0.8873"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_acc", "0.23255813953488372", "0.0148"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_diff", "-7.486188820702225", "0.7876"], ["truthfulqa_mc1", 2.0, "none", 0, "acc", "0.2582619339045288", "0.0153"], ["truthfulqa_mc2", 2.0, "none", 0, "acc", "0.4218454702626678", "0.0143"], ["toxigen", 1.0, "none", null, "acc", "0.5446808510638298", "0.0163"], ["toxigen", 1.0, "none", null, "acc_norm", "0.4319148936170213", "0.0162"], ["mmlu_formal_logic", 0.0, "none", null, "acc", "0.2222222222222222", "0.0372"], ["mmlu_high_school_european_history", 0.0, "none", null, "acc", "0.23636363636363636", "0.0332"], ["mmlu_high_school_us_history", 0.0, "none", null, "acc", "0.23529411764705882", "0.0298"], ["mmlu_high_school_world_history", 0.0, "none", null, "acc", "0.2320675105485232", "0.0275"], ["mmlu_international_law", 0.0, "none", null, "acc", "0.3140495867768595", "0.0424"], ["mmlu_jurisprudence", 0.0, "none", null, "acc", "0.26851851851851855", "0.0428"], ["mmlu_logical_fallacies", 0.0, "none", null, "acc", "0.26993865030674846", "0.0349"], ["mmlu_moral_disputes", 0.0, "none", null, "acc", "0.2630057803468208", "0.0237"], ["mmlu_moral_scenarios", 0.0, "none", null, "acc", "0.23910614525139665", "0.0143"], ["mmlu_philosophy", 0.0, "none", null, "acc", "0.24758842443729903", "0.0245"], ["mmlu_prehistory", 0.0, "none", null, "acc", "0.30246913580246915", "0.0256"], ["mmlu_professional_law", 0.0, "none", null, "acc", "0.25097783572359844", "0.0111"], ["mmlu_world_religions", 0.0, "none", null, "acc", "0.2807017543859649", "0.0345"], ["mmlu_business_ethics", 0.0, "none", null, "acc", "0.18", "0.0386"], ["mmlu_clinical_knowledge", 0.0, "none", null, "acc", "0.2188679245283019", "0.0254"], ["mmlu_college_medicine", 0.0, "none", null, "acc", "0.19653179190751446", "0.0303"], ["mmlu_global_facts", 0.0, "none", null, "acc", "0.33", "0.0473"], ["mmlu_human_aging", 0.0, "none", null, "acc", "0.30493273542600896", "0.0309"], ["mmlu_management", 0.0, "none", null, "acc", "0.22330097087378642", "0.0412"], ["mmlu_marketing", 0.0, "none", null, "acc", "0.2606837606837607", "0.0288"], ["mmlu_medical_genetics", 0.0, "none", null, "acc", "0.27", "0.0446"], ["mmlu_miscellaneous", 0.0, "none", null, "acc", "0.23371647509578544", "0.0151"], ["mmlu_nutrition", 0.0, "none", null, "acc", "0.21568627450980393", "0.0236"], ["mmlu_professional_accounting", 0.0, "none", null, "acc", "0.2553191489361702", "0.0260"], ["mmlu_professional_medicine", 0.0, "none", null, "acc", "0.22794117647058823", "0.0255"], ["mmlu_virology", 0.0, "none", null, "acc", "0.23493975903614459", "0.0330"], ["mmlu_econometrics", 0.0, "none", null, "acc", "0.2631578947368421", "0.0414"], ["mmlu_high_school_geography", 0.0, "none", null, "acc", "0.2474747474747475", "0.0307"], ["mmlu_high_school_government_and_politics", 0.0, "none", null, "acc", "0.23316062176165803", "0.0305"], ["mmlu_high_school_macroeconomics", 0.0, "none", null, "acc", "0.22564102564102564", "0.0212"], ["mmlu_high_school_microeconomics", 0.0, "none", null, "acc", "0.24369747899159663", "0.0279"], ["mmlu_high_school_psychology", 0.0, "none", null, "acc", "0.22018348623853212", "0.0178"], ["mmlu_human_sexuality", 0.0, "none", null, "acc", "0.183206106870229", "0.0339"], ["mmlu_professional_psychology", 0.0, "none", null, "acc", "0.24509803921568626", "0.0174"], ["mmlu_public_relations", 0.0, "none", null, "acc", "0.2727272727272727", "0.0427"], ["mmlu_security_studies", 0.0, "none", null, "acc", "0.22857142857142856", "0.0269"], ["mmlu_sociology", 0.0, "none", null, "acc", "0.24875621890547264", "0.0306"], ["mmlu_us_foreign_policy", 0.0, "none", null, "acc", "0.19", "0.0394"], ["mmlu_abstract_algebra", 0.0, "none", null, "acc", "0.25", "0.0435"], ["mmlu_anatomy", 0.0, "none", null, "acc", "0.28888888888888886", "0.0392"], ["mmlu_astronomy", 0.0, "none", null, "acc", "0.21710526315789475", "0.0336"], ["mmlu_college_biology", 0.0, "none", null, "acc", "0.24305555555555555", "0.0359"], ["mmlu_college_chemistry", 0.0, "none", null, "acc", "0.14", "0.0349"], ["mmlu_college_computer_science", 0.0, "none", null, "acc", "0.23", "0.0423"], ["mmlu_college_mathematics", 0.0, "none", null, "acc", "0.24", "0.0429"], ["mmlu_college_physics", 0.0, "none", null, "acc", "0.2549019607843137", "0.0434"], ["mmlu_computer_security", 0.0, "none", null, "acc", "0.21", "0.0409"], ["mmlu_conceptual_physics", 0.0, "none", null, "acc", "0.2297872340425532", "0.0275"], ["mmlu_electrical_engineering", 0.0, "none", null, "acc", "0.2827586206896552", "0.0375"], ["mmlu_elementary_mathematics", 0.0, "none", null, "acc", "0.25396825396825395", "0.0224"], ["mmlu_high_school_biology", 0.0, "none", null, "acc", "0.25806451612903225", "0.0249"], ["mmlu_high_school_chemistry", 0.0, "none", null, "acc", "0.26108374384236455", "0.0309"], ["mmlu_high_school_computer_science", 0.0, "none", null, "acc", "0.31", "0.0465"], ["mmlu_high_school_mathematics", 0.0, "none", null, "acc", "0.23333333333333334", "0.0258"], ["mmlu_high_school_physics", 0.0, "none", null, "acc", "0.2251655629139073", "0.0341"], ["mmlu_high_school_statistics", 0.0, "none", null, "acc", "0.25462962962962965", "0.0297"], ["mmlu_machine_learning", 0.0, "none", null, "acc", "0.25", "0.0411"], ["hellaswag", 1.0, "none", null, "acc", "0.31099382593108943", "0.0046"], ["hellaswag", 1.0, "none", null, "acc_norm", "0.358195578570006", "0.0048"], ["gsm8k", 3.0, "strict-match", 5, "exact_match", "0.0", "0.0000"], ["gsm8k", 3.0, "flexible-extract", 5, "exact_match", "0.002274450341167551", "0.0013"], ["arc_challenge", 1.0, "none", null, "acc", "0.22781569965870307", "0.0123"], ["arc_challenge", 1.0, "none", null, "acc_norm", "0.28071672354948807", "0.0131"]]}