{"winogrande/alias": "winogrande", "truthfulqa/alias": "truthfulqa", "truthfulqa_gen/alias": " - truthfulqa_gen", "truthfulqa_mc1/alias": " - truthfulqa_mc1", "truthfulqa_mc2/alias": " - truthfulqa_mc2", "toxigen/alias": "toxigen", "mmlu/alias": "mmlu", "mmlu_humanities/alias": " - humanities", "mmlu_formal_logic/alias": "  - formal_logic", "mmlu_high_school_european_history/alias": "  - high_school_european_history", "mmlu_high_school_us_history/alias": "  - high_school_us_history", "mmlu_high_school_world_history/alias": "  - high_school_world_history", "mmlu_international_law/alias": "  - international_law", "mmlu_jurisprudence/alias": "  - jurisprudence", "mmlu_logical_fallacies/alias": "  - logical_fallacies", "mmlu_moral_disputes/alias": "  - moral_disputes", "mmlu_moral_scenarios/alias": "  - moral_scenarios", "mmlu_philosophy/alias": "  - philosophy", "mmlu_prehistory/alias": "  - prehistory", "mmlu_professional_law/alias": "  - professional_law", "mmlu_world_religions/alias": "  - world_religions", "mmlu_other/alias": " - other", "mmlu_business_ethics/alias": "  - business_ethics", "mmlu_clinical_knowledge/alias": "  - clinical_knowledge", "mmlu_college_medicine/alias": "  - college_medicine", "mmlu_global_facts/alias": "  - global_facts", "mmlu_human_aging/alias": "  - human_aging", "mmlu_management/alias": "  - management", "mmlu_marketing/alias": "  - marketing", "mmlu_medical_genetics/alias": "  - medical_genetics", "mmlu_miscellaneous/alias": "  - miscellaneous", "mmlu_nutrition/alias": "  - nutrition", "mmlu_professional_accounting/alias": "  - professional_accounting", "mmlu_professional_medicine/alias": "  - professional_medicine", "mmlu_virology/alias": "  - virology", "mmlu_social_sciences/alias": " - social_sciences", "mmlu_econometrics/alias": "  - econometrics", "mmlu_high_school_geography/alias": "  - high_school_geography", "mmlu_high_school_government_and_politics/alias": "  - high_school_government_and_politics", "mmlu_high_school_macroeconomics/alias": "  - high_school_macroeconomics", "mmlu_high_school_microeconomics/alias": "  - high_school_microeconomics", "mmlu_high_school_psychology/alias": "  - high_school_psychology", "mmlu_human_sexuality/alias": "  - human_sexuality", "mmlu_professional_psychology/alias": "  - professional_psychology", "mmlu_public_relations/alias": "  - public_relations", "mmlu_security_studies/alias": "  - security_studies", "mmlu_sociology/alias": "  - sociology", "mmlu_us_foreign_policy/alias": "  - us_foreign_policy", "mmlu_stem/alias": " - stem", "mmlu_abstract_algebra/alias": "  - abstract_algebra", "mmlu_anatomy/alias": "  - anatomy", "mmlu_astronomy/alias": "  - astronomy", "mmlu_college_biology/alias": "  - college_biology", "mmlu_college_chemistry/alias": "  - college_chemistry", "mmlu_college_computer_science/alias": "  - college_computer_science", "mmlu_college_mathematics/alias": "  - college_mathematics", "mmlu_college_physics/alias": "  - college_physics", "mmlu_computer_security/alias": "  - computer_security", "mmlu_conceptual_physics/alias": "  - conceptual_physics", "mmlu_electrical_engineering/alias": "  - electrical_engineering", "mmlu_elementary_mathematics/alias": "  - elementary_mathematics", "mmlu_high_school_biology/alias": "  - high_school_biology", "mmlu_high_school_chemistry/alias": "  - high_school_chemistry", "mmlu_high_school_computer_science/alias": "  - high_school_computer_science", "mmlu_high_school_mathematics/alias": "  - high_school_mathematics", "mmlu_high_school_physics/alias": "  - high_school_physics", "mmlu_high_school_statistics/alias": "  - high_school_statistics", "mmlu_machine_learning/alias": "  - machine_learning", "hellaswag/alias": "hellaswag", "gsm8k/alias": "gsm8k", "arc_challenge/alias": "arc_challenge", "winogrande/acc": 0.5816890292028414, "winogrande/acc_stderr": 0.013863669961195904, "truthfulqa/rouge2_diff": -6.284275770675301, "truthfulqa/rouge2_diff_stderr": 0.7403635616265406, "truthfulqa/rouge2_acc": 0.12484700122399021, "truthfulqa/rouge2_acc_stderr": 0.01157140171431393, "truthfulqa/rouge1_max": 27.9290055047077, "truthfulqa/rouge1_max_stderr": 0.8283570107546514, "truthfulqa/bleu_acc": 0.4602203182374541, "truthfulqa/bleu_acc_stderr": 0.017448017223960874, "truthfulqa/rougeL_diff": -6.867560065790248, "truthfulqa/rougeL_diff_stderr": 0.8163551377037398, "truthfulqa/bleu_diff": -1.1017812718898787, "truthfulqa/bleu_diff_stderr": 0.49417321966376093, "truthfulqa/rougeL_max": 26.041323044321604, "truthfulqa/rougeL_max_stderr": 0.7963671600335828, "truthfulqa/acc": 0.3473516851243072, "truthfulqa/acc_stderr": 0.01059062959013124, "truthfulqa/rouge1_acc": 0.29008567931456547, "truthfulqa/rouge1_acc_stderr": 0.015886236874209515, "truthfulqa/rouge1_diff": -7.031653808554328, "truthfulqa/rouge1_diff_stderr": 0.8255768014256233, "truthfulqa/rouge2_max": 11.337903181613424, "truthfulqa/rouge2_max_stderr": 0.7560638739520313, "truthfulqa/rougeL_acc": 0.28151774785801714, "truthfulqa/rougeL_acc_stderr": 0.01574402724825605, "truthfulqa/bleu_max": 13.60132784088859, "truthfulqa/bleu_max_stderr": 0.4862381126969418, "truthfulqa_gen/bleu_max": 13.60132784088859, "truthfulqa_gen/bleu_max_stderr": 0.4862381126969418, "truthfulqa_gen/bleu_acc": 0.4602203182374541, "truthfulqa_gen/bleu_acc_stderr": 0.017448017223960874, "truthfulqa_gen/bleu_diff": -1.1017812718898787, "truthfulqa_gen/bleu_diff_stderr": 0.49417321966376093, "truthfulqa_gen/rouge1_max": 27.9290055047077, "truthfulqa_gen/rouge1_max_stderr": 0.8283570107546514, "truthfulqa_gen/rouge1_acc": 0.29008567931456547, "truthfulqa_gen/rouge1_acc_stderr": 0.015886236874209515, "truthfulqa_gen/rouge1_diff": -7.031653808554328, "truthfulqa_gen/rouge1_diff_stderr": 0.8255768014256233, "truthfulqa_gen/rouge2_max": 11.337903181613424, "truthfulqa_gen/rouge2_max_stderr": 0.7560638739520313, "truthfulqa_gen/rouge2_acc": 0.12484700122399021, "truthfulqa_gen/rouge2_acc_stderr": 0.01157140171431393, "truthfulqa_gen/rouge2_diff": -6.284275770675301, "truthfulqa_gen/rouge2_diff_stderr": 0.7403635616265406, "truthfulqa_gen/rougeL_max": 26.041323044321604, "truthfulqa_gen/rougeL_max_stderr": 0.7963671600335828, "truthfulqa_gen/rougeL_acc": 0.28151774785801714, "truthfulqa_gen/rougeL_acc_stderr": 0.01574402724825605, "truthfulqa_gen/rougeL_diff": -6.867560065790248, "truthfulqa_gen/rougeL_diff_stderr": 0.8163551377037398, "truthfulqa_mc1/acc": 0.2668298653610771, "truthfulqa_mc1/acc_stderr": 0.015483691939237256, "truthfulqa_mc2/acc": 0.4278735048875373, "truthfulqa_mc2/acc_stderr": 0.01445340874646022, "toxigen/acc": 0.5680851063829787, "toxigen/acc_stderr": 0.016164899004911828, "toxigen/acc_norm": 0.4319148936170213, "toxigen/acc_norm_stderr": 0.016164899004911828, "mmlu/acc": 0.24020794758581399, "mmlu/acc_stderr": 0.0036024013607651633, "mmlu_humanities/acc": 0.2403825717321998, "mmlu_humanities/acc_stderr": 0.006227662328345214, "mmlu_formal_logic/acc": 0.1746031746031746, "mmlu_formal_logic/acc_stderr": 0.03395490020856113, "mmlu_high_school_european_history/acc": 0.23030303030303031, "mmlu_high_school_european_history/acc_stderr": 0.03287666758603488, "mmlu_high_school_us_history/acc": 0.2647058823529412, "mmlu_high_school_us_history/acc_stderr": 0.03096451792692342, "mmlu_high_school_world_history/acc": 0.21518987341772153, "mmlu_high_school_world_history/acc_stderr": 0.02675082699467617, "mmlu_international_law/acc": 0.24793388429752067, "mmlu_international_law/acc_stderr": 0.03941897526516303, "mmlu_jurisprudence/acc": 0.28703703703703703, "mmlu_jurisprudence/acc_stderr": 0.043733130409147614, "mmlu_logical_fallacies/acc": 0.25153374233128833, "mmlu_logical_fallacies/acc_stderr": 0.034089978868575295, "mmlu_moral_disputes/acc": 0.25722543352601157, "mmlu_moral_disputes/acc_stderr": 0.02353292543104429, "mmlu_moral_scenarios/acc": 0.2424581005586592, "mmlu_moral_scenarios/acc_stderr": 0.014333522059217887, "mmlu_philosophy/acc": 0.1864951768488746, "mmlu_philosophy/acc_stderr": 0.02212243977248077, "mmlu_prehistory/acc": 0.2345679012345679, "mmlu_prehistory/acc_stderr": 0.02357688174400572, "mmlu_professional_law/acc": 0.24185136897001303, "mmlu_professional_law/acc_stderr": 0.010936550813827054, "mmlu_world_religions/acc": 0.30994152046783624, "mmlu_world_religions/acc_stderr": 0.035469769593931624, "mmlu_other/acc": 0.24975860959124557, "mmlu_other/acc_stderr": 0.007747719663670339, "mmlu_business_ethics/acc": 0.25, "mmlu_business_ethics/acc_stderr": 0.04351941398892446, "mmlu_clinical_knowledge/acc": 0.23773584905660378, "mmlu_clinical_knowledge/acc_stderr": 0.02619980880756192, "mmlu_college_medicine/acc": 0.17341040462427745, "mmlu_college_medicine/acc_stderr": 0.02886810787497064, "mmlu_global_facts/acc": 0.31, "mmlu_global_facts/acc_stderr": 0.04648231987117316, "mmlu_human_aging/acc": 0.35874439461883406, "mmlu_human_aging/acc_stderr": 0.03219079200419995, "mmlu_management/acc": 0.1941747572815534, "mmlu_management/acc_stderr": 0.03916667762822585, "mmlu_marketing/acc": 0.25213675213675213, "mmlu_marketing/acc_stderr": 0.02844796547623101, "mmlu_medical_genetics/acc": 0.29, "mmlu_medical_genetics/acc_stderr": 0.045604802157206845, "mmlu_miscellaneous/acc": 0.2554278416347382, "mmlu_miscellaneous/acc_stderr": 0.015594955384455758, "mmlu_nutrition/acc": 0.21241830065359477, "mmlu_nutrition/acc_stderr": 0.02342037547829613, "mmlu_professional_accounting/acc": 0.2553191489361702, "mmlu_professional_accounting/acc_stderr": 0.026011992930902006, "mmlu_professional_medicine/acc": 0.22794117647058823, "mmlu_professional_medicine/acc_stderr": 0.0254830814680298, "mmlu_virology/acc": 0.24096385542168675, "mmlu_virology/acc_stderr": 0.0332939411907353, "mmlu_social_sciences/acc": 0.22619434514137146, "mmlu_social_sciences/acc_stderr": 0.007543402019660995, "mmlu_econometrics/acc": 0.20175438596491227, "mmlu_econometrics/acc_stderr": 0.037752050135836386, "mmlu_high_school_geography/acc": 0.21717171717171718, "mmlu_high_school_geography/acc_stderr": 0.029376616484945637, "mmlu_high_school_government_and_politics/acc": 0.2538860103626943, "mmlu_high_school_government_and_politics/acc_stderr": 0.0314102478056532, "mmlu_high_school_macroeconomics/acc": 0.2230769230769231, "mmlu_high_school_macroeconomics/acc_stderr": 0.021107730127243998, "mmlu_high_school_microeconomics/acc": 0.21428571428571427, "mmlu_high_school_microeconomics/acc_stderr": 0.026653531596715477, "mmlu_high_school_psychology/acc": 0.23853211009174313, "mmlu_high_school_psychology/acc_stderr": 0.018272575810231867, "mmlu_human_sexuality/acc": 0.16030534351145037, "mmlu_human_sexuality/acc_stderr": 0.0321782942074463, "mmlu_professional_psychology/acc": 0.25163398692810457, "mmlu_professional_psychology/acc_stderr": 0.017555818091322256, "mmlu_public_relations/acc": 0.22727272727272727, "mmlu_public_relations/acc_stderr": 0.04013964554072774, "mmlu_security_studies/acc": 0.17959183673469387, "mmlu_security_studies/acc_stderr": 0.024573293589585637, "mmlu_sociology/acc": 0.21890547263681592, "mmlu_sociology/acc_stderr": 0.029239174636647, "mmlu_us_foreign_policy/acc": 0.25, "mmlu_us_foreign_policy/acc_stderr": 0.04351941398892446, "mmlu_stem/acc": 0.24421186171899778, "mmlu_stem/acc_stderr": 0.007651698907329533, "mmlu_abstract_algebra/acc": 0.26, "mmlu_abstract_algebra/acc_stderr": 0.044084400227680794, "mmlu_anatomy/acc": 0.25925925925925924, "mmlu_anatomy/acc_stderr": 0.037857144650666516, "mmlu_astronomy/acc": 0.19736842105263158, "mmlu_astronomy/acc_stderr": 0.03238981601699397, "mmlu_college_biology/acc": 0.2361111111111111, "mmlu_college_biology/acc_stderr": 0.03551446610810826, "mmlu_college_chemistry/acc": 0.15, "mmlu_college_chemistry/acc_stderr": 0.03588702812826372, "mmlu_college_computer_science/acc": 0.27, "mmlu_college_computer_science/acc_stderr": 0.04461960433384741, "mmlu_college_mathematics/acc": 0.28, "mmlu_college_mathematics/acc_stderr": 0.04512608598542128, "mmlu_college_physics/acc": 0.2549019607843137, "mmlu_college_physics/acc_stderr": 0.043364327079931785, "mmlu_computer_security/acc": 0.25, "mmlu_computer_security/acc_stderr": 0.04351941398892446, "mmlu_conceptual_physics/acc": 0.25957446808510637, "mmlu_conceptual_physics/acc_stderr": 0.02865917937429232, "mmlu_electrical_engineering/acc": 0.2620689655172414, "mmlu_electrical_engineering/acc_stderr": 0.036646663372252565, "mmlu_elementary_mathematics/acc": 0.2830687830687831, "mmlu_elementary_mathematics/acc_stderr": 0.023201392938194974, "mmlu_high_school_biology/acc": 0.25483870967741934, "mmlu_high_school_biology/acc_stderr": 0.024790118459332208, "mmlu_high_school_chemistry/acc": 0.2561576354679803, "mmlu_high_school_chemistry/acc_stderr": 0.030712730070982592, "mmlu_high_school_computer_science/acc": 0.26, "mmlu_high_school_computer_science/acc_stderr": 0.04408440022768081, "mmlu_high_school_mathematics/acc": 0.21481481481481482, "mmlu_high_school_mathematics/acc_stderr": 0.02504044387700069, "mmlu_high_school_physics/acc": 0.17880794701986755, "mmlu_high_school_physics/acc_stderr": 0.03128744850600724, "mmlu_high_school_statistics/acc": 0.21296296296296297, "mmlu_high_school_statistics/acc_stderr": 0.027920963147993666, "mmlu_machine_learning/acc": 0.26785714285714285, "mmlu_machine_learning/acc_stderr": 0.04203277291467762, "hellaswag/acc": 0.3232423819956184, "hellaswag/acc_stderr": 0.0046675850727174745, "hellaswag/acc_norm": 0.3646683927504481, "hellaswag/acc_norm_stderr": 0.004803533333364235, "gsm8k/exact_match,strict-match": 0.0, "gsm8k/exact_match_stderr,strict-match": 0.0, "gsm8k/exact_match,flexible-extract": 0.0075815011372251705, "gsm8k/exact_match_stderr,flexible-extract": 0.0023892815120772574, "arc_challenge/acc": 0.22781569965870307, "arc_challenge/acc_stderr": 0.012256708602326936, "arc_challenge/acc_norm": 0.2713310580204778, "arc_challenge/acc_norm_stderr": 0.012993807727545803, "_timestamp": 1711102468.0159624, "_runtime": 2133.621839284897, "_step": 2, "evaluation/eval_results": {"_type": "table-file", "sha256": "2219d2bba4124a666cf3de77143a096e87231a217436a2c82c8eceb4c33f9272", "size": 6725, "path": "media/table/evaluation/eval_results_1_2219d2bba4124a666cf3.table.json", "ncols": 7, "nrows": 80}, "evaluation/group_eval_results": {"_type": "table-file", "sha256": "b34df168ecf1d9a762ef42e5edf971f8f677834b7dc83c6004496f33f4dee580", "size": 1501, "path": "media/table/evaluation/group_eval_results_2_b34df168ecf1d9a762ef.table.json", "ncols": 7, "nrows": 18}, "_wandb": {"runtime": 2133}}