{"winogrande/alias": "winogrande", "truthfulqa/alias": "truthfulqa", "truthfulqa_gen/alias": " - truthfulqa_gen", "truthfulqa_mc1/alias": " - truthfulqa_mc1", "truthfulqa_mc2/alias": " - truthfulqa_mc2", "toxigen/alias": "toxigen", "mmlu/alias": "mmlu", "mmlu_humanities/alias": " - humanities", "mmlu_formal_logic/alias": "  - formal_logic", "mmlu_high_school_european_history/alias": "  - high_school_european_history", "mmlu_high_school_us_history/alias": "  - high_school_us_history", "mmlu_high_school_world_history/alias": "  - high_school_world_history", "mmlu_international_law/alias": "  - international_law", "mmlu_jurisprudence/alias": "  - jurisprudence", "mmlu_logical_fallacies/alias": "  - logical_fallacies", "mmlu_moral_disputes/alias": "  - moral_disputes", "mmlu_moral_scenarios/alias": "  - moral_scenarios", "mmlu_philosophy/alias": "  - philosophy", "mmlu_prehistory/alias": "  - prehistory", "mmlu_professional_law/alias": "  - professional_law", "mmlu_world_religions/alias": "  - world_religions", "mmlu_other/alias": " - other", "mmlu_business_ethics/alias": "  - business_ethics", "mmlu_clinical_knowledge/alias": "  - clinical_knowledge", "mmlu_college_medicine/alias": "  - college_medicine", "mmlu_global_facts/alias": "  - global_facts", "mmlu_human_aging/alias": "  - human_aging", "mmlu_management/alias": "  - management", "mmlu_marketing/alias": "  - marketing", "mmlu_medical_genetics/alias": "  - medical_genetics", "mmlu_miscellaneous/alias": "  - miscellaneous", "mmlu_nutrition/alias": "  - nutrition", "mmlu_professional_accounting/alias": "  - professional_accounting", "mmlu_professional_medicine/alias": "  - professional_medicine", "mmlu_virology/alias": "  - virology", "mmlu_social_sciences/alias": " - social_sciences", "mmlu_econometrics/alias": "  - econometrics", "mmlu_high_school_geography/alias": "  - high_school_geography", "mmlu_high_school_government_and_politics/alias": "  - high_school_government_and_politics", "mmlu_high_school_macroeconomics/alias": "  - high_school_macroeconomics", "mmlu_high_school_microeconomics/alias": "  - high_school_microeconomics", "mmlu_high_school_psychology/alias": "  - high_school_psychology", "mmlu_human_sexuality/alias": "  - human_sexuality", "mmlu_professional_psychology/alias": "  - professional_psychology", "mmlu_public_relations/alias": "  - public_relations", "mmlu_security_studies/alias": "  - security_studies", "mmlu_sociology/alias": "  - sociology", "mmlu_us_foreign_policy/alias": "  - us_foreign_policy", "mmlu_stem/alias": " - stem", "mmlu_abstract_algebra/alias": "  - abstract_algebra", "mmlu_anatomy/alias": "  - anatomy", "mmlu_astronomy/alias": "  - astronomy", "mmlu_college_biology/alias": "  - college_biology", "mmlu_college_chemistry/alias": "  - college_chemistry", "mmlu_college_computer_science/alias": "  - college_computer_science", "mmlu_college_mathematics/alias": "  - college_mathematics", "mmlu_college_physics/alias": "  - college_physics", "mmlu_computer_security/alias": "  - computer_security", "mmlu_conceptual_physics/alias": "  - conceptual_physics", "mmlu_electrical_engineering/alias": "  - electrical_engineering", "mmlu_elementary_mathematics/alias": "  - elementary_mathematics", "mmlu_high_school_biology/alias": "  - high_school_biology", "mmlu_high_school_chemistry/alias": "  - high_school_chemistry", "mmlu_high_school_computer_science/alias": "  - high_school_computer_science", "mmlu_high_school_mathematics/alias": "  - high_school_mathematics", "mmlu_high_school_physics/alias": "  - high_school_physics", "mmlu_high_school_statistics/alias": "  - high_school_statistics", "mmlu_machine_learning/alias": "  - machine_learning", "hellaswag/alias": "hellaswag", "gsm8k/alias": "gsm8k", "arc_challenge/alias": "arc_challenge", "winogrande/acc": 0.584846093133386, "winogrande/acc_stderr": 0.013848684086658585, "truthfulqa/rouge1_diff": -7.004123193332931, "truthfulqa/rouge1_diff_stderr": 0.8670373275777925, "truthfulqa/rouge2_diff": -6.2852398329431916, "truthfulqa/rouge2_diff_stderr": 0.8101142808539338, "truthfulqa/bleu_acc": 0.45532435740514077, "truthfulqa/bleu_acc_stderr": 0.017433490102538758, "truthfulqa/rougeL_max": 28.261723055541836, "truthfulqa/rougeL_max_stderr": 0.8321901548869074, "truthfulqa/rouge1_max": 30.21788137418243, "truthfulqa/rouge1_max_stderr": 0.8600529690348334, "truthfulqa/rouge1_acc": 0.28886168910648713, "truthfulqa/rouge1_acc_stderr": 0.01586634640138431, "truthfulqa/rouge2_max": 13.635575887034115, "truthfulqa/rouge2_max_stderr": 0.8121006836144672, "truthfulqa/rougeL_acc": 0.2766217870257038, "truthfulqa/rougeL_acc_stderr": 0.015659605755326933, "truthfulqa/rougeL_diff": -6.674786607787514, "truthfulqa/rougeL_diff_stderr": 0.8540771359578861, "truthfulqa/bleu_diff": -1.4212580118949159, "truthfulqa/bleu_diff_stderr": 0.5551942767819699, "truthfulqa/acc": 0.34094539222982456, "truthfulqa/acc_stderr": 0.010561437268463439, "truthfulqa/bleu_max": 14.635148587987521, "truthfulqa/bleu_max_stderr": 0.5241076440403869, "truthfulqa/rouge2_acc": 0.14443084455324356, "truthfulqa/rouge2_acc_stderr": 0.01230587376165713, "truthfulqa_gen/bleu_max": 14.635148587987521, "truthfulqa_gen/bleu_max_stderr": 0.5241076440403869, "truthfulqa_gen/bleu_acc": 0.45532435740514077, "truthfulqa_gen/bleu_acc_stderr": 0.017433490102538758, "truthfulqa_gen/bleu_diff": -1.4212580118949159, "truthfulqa_gen/bleu_diff_stderr": 0.5551942767819699, "truthfulqa_gen/rouge1_max": 30.21788137418243, "truthfulqa_gen/rouge1_max_stderr": 0.8600529690348334, "truthfulqa_gen/rouge1_acc": 0.28886168910648713, "truthfulqa_gen/rouge1_acc_stderr": 0.01586634640138431, "truthfulqa_gen/rouge1_diff": -7.004123193332931, "truthfulqa_gen/rouge1_diff_stderr": 0.8670373275777924, "truthfulqa_gen/rouge2_max": 13.635575887034115, "truthfulqa_gen/rouge2_max_stderr": 0.8121006836144672, "truthfulqa_gen/rouge2_acc": 0.14443084455324356, "truthfulqa_gen/rouge2_acc_stderr": 0.01230587376165713, "truthfulqa_gen/rouge2_diff": -6.2852398329431916, "truthfulqa_gen/rouge2_diff_stderr": 0.8101142808539339, "truthfulqa_gen/rougeL_max": 28.261723055541836, "truthfulqa_gen/rougeL_max_stderr": 0.8321901548869074, "truthfulqa_gen/rougeL_acc": 0.2766217870257038, "truthfulqa_gen/rougeL_acc_stderr": 0.015659605755326933, "truthfulqa_gen/rougeL_diff": -6.674786607787514, "truthfulqa_gen/rougeL_diff_stderr": 0.8540771359578863, "truthfulqa_mc1/acc": 0.2582619339045288, "truthfulqa_mc1/acc_stderr": 0.015321821688476185, "truthfulqa_mc2/acc": 0.42362885055512034, "truthfulqa_mc2/acc_stderr": 0.01454020663021316, "toxigen/acc": 0.5659574468085107, "toxigen/acc_stderr": 0.016174290832374864, "toxigen/acc_norm": 0.4308510638297872, "toxigen/acc_norm_stderr": 0.016160089171486036, "mmlu/acc": 0.2355077624270047, "mmlu/acc_stderr": 0.003577032685021643, "mmlu_humanities/acc": 0.23761955366631243, "mmlu_humanities/acc_stderr": 0.006203168842467547, "mmlu_formal_logic/acc": 0.2222222222222222, "mmlu_formal_logic/acc_stderr": 0.037184890068181146, "mmlu_high_school_european_history/acc": 0.21212121212121213, "mmlu_high_school_european_history/acc_stderr": 0.03192271569548299, "mmlu_high_school_us_history/acc": 0.25980392156862747, "mmlu_high_school_us_history/acc_stderr": 0.030778554678693268, "mmlu_high_school_world_history/acc": 0.21518987341772153, "mmlu_high_school_world_history/acc_stderr": 0.026750826994676173, "mmlu_international_law/acc": 0.2644628099173554, "mmlu_international_law/acc_stderr": 0.04026187527591205, "mmlu_jurisprudence/acc": 0.2777777777777778, "mmlu_jurisprudence/acc_stderr": 0.04330043749650742, "mmlu_logical_fallacies/acc": 0.2147239263803681, "mmlu_logical_fallacies/acc_stderr": 0.03226219377286774, "mmlu_moral_disputes/acc": 0.24277456647398843, "mmlu_moral_disputes/acc_stderr": 0.0230836585869842, "mmlu_moral_scenarios/acc": 0.2424581005586592, "mmlu_moral_scenarios/acc_stderr": 0.014333522059217887, "mmlu_philosophy/acc": 0.18006430868167203, "mmlu_philosophy/acc_stderr": 0.021823422857744953, "mmlu_prehistory/acc": 0.22530864197530864, "mmlu_prehistory/acc_stderr": 0.02324620264781975, "mmlu_professional_law/acc": 0.24119947848761408, "mmlu_professional_law/acc_stderr": 0.01092649610203496, "mmlu_world_religions/acc": 0.3157894736842105, "mmlu_world_religions/acc_stderr": 0.03565079670708311, "mmlu_other/acc": 0.2452526552944963, "mmlu_other/acc_stderr": 0.007698919754165841, "mmlu_business_ethics/acc": 0.27, "mmlu_business_ethics/acc_stderr": 0.04461960433384739, "mmlu_clinical_knowledge/acc": 0.25660377358490566, "mmlu_clinical_knowledge/acc_stderr": 0.02688064788905199, "mmlu_college_medicine/acc": 0.2023121387283237, "mmlu_college_medicine/acc_stderr": 0.030631145539198823, "mmlu_global_facts/acc": 0.26, "mmlu_global_facts/acc_stderr": 0.04408440022768079, "mmlu_human_aging/acc": 0.3632286995515695, "mmlu_human_aging/acc_stderr": 0.03227790442850499, "mmlu_management/acc": 0.20388349514563106, "mmlu_management/acc_stderr": 0.03989139859531771, "mmlu_marketing/acc": 0.23076923076923078, "mmlu_marketing/acc_stderr": 0.02760192138141759, "mmlu_medical_genetics/acc": 0.26, "mmlu_medical_genetics/acc_stderr": 0.04408440022768079, "mmlu_miscellaneous/acc": 0.2247765006385696, "mmlu_miscellaneous/acc_stderr": 0.01492744710193715, "mmlu_nutrition/acc": 0.19934640522875818, "mmlu_nutrition/acc_stderr": 0.022875816993464075, "mmlu_professional_accounting/acc": 0.2553191489361702, "mmlu_professional_accounting/acc_stderr": 0.026011992930902, "mmlu_professional_medicine/acc": 0.23897058823529413, "mmlu_professional_medicine/acc_stderr": 0.025905280644893006, "mmlu_virology/acc": 0.30120481927710846, "mmlu_virology/acc_stderr": 0.0357160923005348, "mmlu_social_sciences/acc": 0.21936951576210595, "mmlu_social_sciences/acc_stderr": 0.007464152860285104, "mmlu_econometrics/acc": 0.20175438596491227, "mmlu_econometrics/acc_stderr": 0.03775205013583639, "mmlu_high_school_geography/acc": 0.21212121212121213, "mmlu_high_school_geography/acc_stderr": 0.02912652283458682, "mmlu_high_school_government_and_politics/acc": 0.21243523316062177, "mmlu_high_school_government_and_politics/acc_stderr": 0.02951928261681726, "mmlu_high_school_macroeconomics/acc": 0.2230769230769231, "mmlu_high_school_macroeconomics/acc_stderr": 0.021107730127243998, "mmlu_high_school_microeconomics/acc": 0.21008403361344538, "mmlu_high_school_microeconomics/acc_stderr": 0.026461398717471874, "mmlu_high_school_psychology/acc": 0.23302752293577983, "mmlu_high_school_psychology/acc_stderr": 0.0181256691808615, "mmlu_human_sexuality/acc": 0.17557251908396945, "mmlu_human_sexuality/acc_stderr": 0.03336820338476076, "mmlu_professional_psychology/acc": 0.24183006535947713, "mmlu_professional_psychology/acc_stderr": 0.017322789207784326, "mmlu_public_relations/acc": 0.21818181818181817, "mmlu_public_relations/acc_stderr": 0.03955932861795833, "mmlu_security_studies/acc": 0.1673469387755102, "mmlu_security_studies/acc_stderr": 0.02389714476891452, "mmlu_sociology/acc": 0.23880597014925373, "mmlu_sociology/acc_stderr": 0.030147775935409224, "mmlu_us_foreign_policy/acc": 0.21, "mmlu_us_foreign_policy/acc_stderr": 0.04093601807403326, "mmlu_stem/acc": 0.23850301300348875, "mmlu_stem/acc_stderr": 0.007581468202274038, "mmlu_abstract_algebra/acc": 0.29, "mmlu_abstract_algebra/acc_stderr": 0.04560480215720684, "mmlu_anatomy/acc": 0.23703703703703705, "mmlu_anatomy/acc_stderr": 0.03673731683969506, "mmlu_astronomy/acc": 0.13815789473684212, "mmlu_astronomy/acc_stderr": 0.028081042939576552, "mmlu_college_biology/acc": 0.2569444444444444, "mmlu_college_biology/acc_stderr": 0.036539469694421, "mmlu_college_chemistry/acc": 0.14, "mmlu_college_chemistry/acc_stderr": 0.03487350880197769, "mmlu_college_computer_science/acc": 0.24, "mmlu_college_computer_science/acc_stderr": 0.04292346959909282, "mmlu_college_mathematics/acc": 0.29, "mmlu_college_mathematics/acc_stderr": 0.045604802157206845, "mmlu_college_physics/acc": 0.24509803921568626, "mmlu_college_physics/acc_stderr": 0.04280105837364395, "mmlu_computer_security/acc": 0.22, "mmlu_computer_security/acc_stderr": 0.04163331998932269, "mmlu_conceptual_physics/acc": 0.23404255319148937, "mmlu_conceptual_physics/acc_stderr": 0.027678452578212387, "mmlu_electrical_engineering/acc": 0.25517241379310346, "mmlu_electrical_engineering/acc_stderr": 0.03632984052707841, "mmlu_elementary_mathematics/acc": 0.2830687830687831, "mmlu_elementary_mathematics/acc_stderr": 0.023201392938194974, "mmlu_high_school_biology/acc": 0.23548387096774193, "mmlu_high_school_biology/acc_stderr": 0.024137632429337707, "mmlu_high_school_chemistry/acc": 0.2512315270935961, "mmlu_high_school_chemistry/acc_stderr": 0.030516530732694436, "mmlu_high_school_computer_science/acc": 0.27, "mmlu_high_school_computer_science/acc_stderr": 0.04461960433384741, "mmlu_high_school_mathematics/acc": 0.24074074074074073, "mmlu_high_school_mathematics/acc_stderr": 0.026067159222275784, "mmlu_high_school_physics/acc": 0.19205298013245034, "mmlu_high_school_physics/acc_stderr": 0.03216298420593614, "mmlu_high_school_statistics/acc": 0.20833333333333334, "mmlu_high_school_statistics/acc_stderr": 0.027696910713093933, "mmlu_machine_learning/acc": 0.26785714285714285, "mmlu_machine_learning/acc_stderr": 0.04203277291467762, "hellaswag/acc": 0.32642899820752835, "hellaswag/acc_stderr": 0.004679479763516797, "hellaswag/acc_norm": 0.3776140211113324, "hellaswag/acc_norm_stderr": 0.004837995637638562, "gsm8k/exact_match,strict-match": 0.0, "gsm8k/exact_match_stderr,strict-match": 0.0, "gsm8k/exact_match,flexible-extract": 0.009097801364670205, "gsm8k/exact_match_stderr,flexible-extract": 0.002615326510775673, "arc_challenge/acc": 0.22610921501706485, "arc_challenge/acc_stderr": 0.012224202097063297, "arc_challenge/acc_norm": 0.2738907849829352, "arc_challenge/acc_norm_stderr": 0.013032004972989503, "_timestamp": 1711100322.1988063, "_runtime": 2113.03124833107, "_step": 2, "evaluation/eval_results": {"_type": "table-file", "sha256": "532a8d0d513cf73f6a9cf2fe209d77ce74490c80d6f6f67d37a2d663829aa050", "size": 6728, "path": "media/table/evaluation/eval_results_1_532a8d0d513cf73f6a9c.table.json", "ncols": 7, "nrows": 80}, "evaluation/group_eval_results": {"_type": "table-file", "sha256": "c82724e806106b9873b758235a357291d448ca1a79f218b09bee36069bfb0d1e", "size": 1504, "path": "media/table/evaluation/group_eval_results_2_c82724e806106b9873b7.table.json", "ncols": 7, "nrows": 18}, "_wandb": {"runtime": 2113}}