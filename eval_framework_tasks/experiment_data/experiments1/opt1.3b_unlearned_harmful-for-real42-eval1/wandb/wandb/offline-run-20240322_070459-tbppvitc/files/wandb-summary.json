{"winogrande/alias": "winogrande", "truthfulqa/alias": "truthfulqa", "truthfulqa_gen/alias": " - truthfulqa_gen", "truthfulqa_mc1/alias": " - truthfulqa_mc1", "truthfulqa_mc2/alias": " - truthfulqa_mc2", "toxigen/alias": "toxigen", "mmlu/alias": "mmlu", "mmlu_humanities/alias": " - humanities", "mmlu_formal_logic/alias": "  - formal_logic", "mmlu_high_school_european_history/alias": "  - high_school_european_history", "mmlu_high_school_us_history/alias": "  - high_school_us_history", "mmlu_high_school_world_history/alias": "  - high_school_world_history", "mmlu_international_law/alias": "  - international_law", "mmlu_jurisprudence/alias": "  - jurisprudence", "mmlu_logical_fallacies/alias": "  - logical_fallacies", "mmlu_moral_disputes/alias": "  - moral_disputes", "mmlu_moral_scenarios/alias": "  - moral_scenarios", "mmlu_philosophy/alias": "  - philosophy", "mmlu_prehistory/alias": "  - prehistory", "mmlu_professional_law/alias": "  - professional_law", "mmlu_world_religions/alias": "  - world_religions", "mmlu_other/alias": " - other", "mmlu_business_ethics/alias": "  - business_ethics", "mmlu_clinical_knowledge/alias": "  - clinical_knowledge", "mmlu_college_medicine/alias": "  - college_medicine", "mmlu_global_facts/alias": "  - global_facts", "mmlu_human_aging/alias": "  - human_aging", "mmlu_management/alias": "  - management", "mmlu_marketing/alias": "  - marketing", "mmlu_medical_genetics/alias": "  - medical_genetics", "mmlu_miscellaneous/alias": "  - miscellaneous", "mmlu_nutrition/alias": "  - nutrition", "mmlu_professional_accounting/alias": "  - professional_accounting", "mmlu_professional_medicine/alias": "  - professional_medicine", "mmlu_virology/alias": "  - virology", "mmlu_social_sciences/alias": " - social_sciences", "mmlu_econometrics/alias": "  - econometrics", "mmlu_high_school_geography/alias": "  - high_school_geography", "mmlu_high_school_government_and_politics/alias": "  - high_school_government_and_politics", "mmlu_high_school_macroeconomics/alias": "  - high_school_macroeconomics", "mmlu_high_school_microeconomics/alias": "  - high_school_microeconomics", "mmlu_high_school_psychology/alias": "  - high_school_psychology", "mmlu_human_sexuality/alias": "  - human_sexuality", "mmlu_professional_psychology/alias": "  - professional_psychology", "mmlu_public_relations/alias": "  - public_relations", "mmlu_security_studies/alias": "  - security_studies", "mmlu_sociology/alias": "  - sociology", "mmlu_us_foreign_policy/alias": "  - us_foreign_policy", "mmlu_stem/alias": " - stem", "mmlu_abstract_algebra/alias": "  - abstract_algebra", "mmlu_anatomy/alias": "  - anatomy", "mmlu_astronomy/alias": "  - astronomy", "mmlu_college_biology/alias": "  - college_biology", "mmlu_college_chemistry/alias": "  - college_chemistry", "mmlu_college_computer_science/alias": "  - college_computer_science", "mmlu_college_mathematics/alias": "  - college_mathematics", "mmlu_college_physics/alias": "  - college_physics", "mmlu_computer_security/alias": "  - computer_security", "mmlu_conceptual_physics/alias": "  - conceptual_physics", "mmlu_electrical_engineering/alias": "  - electrical_engineering", "mmlu_elementary_mathematics/alias": "  - elementary_mathematics", "mmlu_high_school_biology/alias": "  - high_school_biology", "mmlu_high_school_chemistry/alias": "  - high_school_chemistry", "mmlu_high_school_computer_science/alias": "  - high_school_computer_science", "mmlu_high_school_mathematics/alias": "  - high_school_mathematics", "mmlu_high_school_physics/alias": "  - high_school_physics", "mmlu_high_school_statistics/alias": "  - high_school_statistics", "mmlu_machine_learning/alias": "  - machine_learning", "hellaswag/alias": "hellaswag", "gsm8k/alias": "gsm8k", "arc_challenge/alias": "arc_challenge", "winogrande/acc": 0.5951065509076559, "winogrande/acc_stderr": 0.013795927003124946, "truthfulqa/rouge1_acc": 0.2668298653610771, "truthfulqa/rouge1_acc_stderr": 0.015483691939237272, "truthfulqa/rougeL_max": 37.784413490481995, "truthfulqa/rougeL_max_stderr": 0.8947615693085174, "truthfulqa/rouge2_diff": -11.378988631456103, "truthfulqa/rouge2_diff_stderr": 0.9012694800889249, "truthfulqa/rouge1_diff": -9.63075083263681, "truthfulqa/rouge1_diff_stderr": 0.8420994530840882, "truthfulqa/rougeL_acc": 0.24357405140758873, "truthfulqa/rougeL_acc_stderr": 0.015026354824910782, "truthfulqa/bleu_acc": 0.3378212974296206, "truthfulqa/bleu_acc_stderr": 0.016557167322516865, "truthfulqa/rouge2_max": 23.827561343053592, "truthfulqa/rouge2_max_stderr": 0.9275084466012008, "truthfulqa/bleu_diff": -5.857677666844719, "truthfulqa/bleu_diff_stderr": 0.6553728778344077, "truthfulqa/rougeL_diff": -9.705907462481314, "truthfulqa/rougeL_diff_stderr": 0.8258519565044679, "truthfulqa/bleu_max": 19.13842800888533, "truthfulqa/bleu_max_stderr": 0.6652487099198096, "truthfulqa/rouge1_max": 40.34013865893567, "truthfulqa/rouge1_max_stderr": 0.9053366384112681, "truthfulqa/rouge2_acc": 0.17870257037943696, "truthfulqa/rouge2_acc_stderr": 0.01341128995232445, "truthfulqa/acc": 0.3120798939676711, "truthfulqa/acc_stderr": 0.010295988214942792, "truthfulqa_gen/bleu_max": 19.13842800888533, "truthfulqa_gen/bleu_max_stderr": 0.6652487099198096, "truthfulqa_gen/bleu_acc": 0.3378212974296206, "truthfulqa_gen/bleu_acc_stderr": 0.016557167322516865, "truthfulqa_gen/bleu_diff": -5.857677666844719, "truthfulqa_gen/bleu_diff_stderr": 0.6553728778344077, "truthfulqa_gen/rouge1_max": 40.34013865893567, "truthfulqa_gen/rouge1_max_stderr": 0.9053366384112681, "truthfulqa_gen/rouge1_acc": 0.2668298653610771, "truthfulqa_gen/rouge1_acc_stderr": 0.015483691939237272, "truthfulqa_gen/rouge1_diff": -9.63075083263681, "truthfulqa_gen/rouge1_diff_stderr": 0.8420994530840882, "truthfulqa_gen/rouge2_max": 23.827561343053592, "truthfulqa_gen/rouge2_max_stderr": 0.9275084466012008, "truthfulqa_gen/rouge2_acc": 0.17870257037943696, "truthfulqa_gen/rouge2_acc_stderr": 0.01341128995232445, "truthfulqa_gen/rouge2_diff": -11.378988631456103, "truthfulqa_gen/rouge2_diff_stderr": 0.9012694800889249, "truthfulqa_gen/rougeL_max": 37.784413490481995, "truthfulqa_gen/rougeL_max_stderr": 0.8947615693085174, "truthfulqa_gen/rougeL_acc": 0.24357405140758873, "truthfulqa_gen/rougeL_acc_stderr": 0.015026354824910782, "truthfulqa_gen/rougeL_diff": -9.705907462481314, "truthfulqa_gen/rougeL_diff_stderr": 0.8258519565044679, "truthfulqa_mc1/acc": 0.23745410036719705, "truthfulqa_mc1/acc_stderr": 0.014896277441041845, "truthfulqa_mc2/acc": 0.38670568756814516, "truthfulqa_mc2/acc_stderr": 0.014217257530496914, "toxigen/acc": 0.5414893617021277, "toxigen/acc_stderr": 0.01626061160410856, "toxigen/acc_norm": 0.4308510638297872, "toxigen/acc_norm_stderr": 0.016160089171486036, "mmlu/acc": 0.2491810283435408, "mmlu/acc_stderr": 0.003647834253794939, "mmlu_humanities/acc": 0.2561105207226355, "mmlu_humanities/acc_stderr": 0.006361533366798408, "mmlu_formal_logic/acc": 0.2222222222222222, "mmlu_formal_logic/acc_stderr": 0.03718489006818114, "mmlu_high_school_european_history/acc": 0.24848484848484848, "mmlu_high_school_european_history/acc_stderr": 0.03374402644139404, "mmlu_high_school_us_history/acc": 0.25, "mmlu_high_school_us_history/acc_stderr": 0.03039153369274154, "mmlu_high_school_world_history/acc": 0.22362869198312235, "mmlu_high_school_world_history/acc_stderr": 0.02712329820522997, "mmlu_international_law/acc": 0.34710743801652894, "mmlu_international_law/acc_stderr": 0.04345724570292535, "mmlu_jurisprudence/acc": 0.2962962962962963, "mmlu_jurisprudence/acc_stderr": 0.044143436668549335, "mmlu_logical_fallacies/acc": 0.26993865030674846, "mmlu_logical_fallacies/acc_stderr": 0.034878251684978906, "mmlu_moral_disputes/acc": 0.2861271676300578, "mmlu_moral_disputes/acc_stderr": 0.024332146779134135, "mmlu_moral_scenarios/acc": 0.24134078212290502, "mmlu_moral_scenarios/acc_stderr": 0.014310999547961464, "mmlu_philosophy/acc": 0.2282958199356913, "mmlu_philosophy/acc_stderr": 0.023839303311398212, "mmlu_prehistory/acc": 0.2993827160493827, "mmlu_prehistory/acc_stderr": 0.025483115601195462, "mmlu_professional_law/acc": 0.2522816166883963, "mmlu_professional_law/acc_stderr": 0.011092789056875229, "mmlu_world_religions/acc": 0.2573099415204678, "mmlu_world_religions/acc_stderr": 0.03352799844161865, "mmlu_other/acc": 0.24654007080785323, "mmlu_other/acc_stderr": 0.007721185251743877, "mmlu_business_ethics/acc": 0.26, "mmlu_business_ethics/acc_stderr": 0.044084400227680794, "mmlu_clinical_knowledge/acc": 0.21132075471698114, "mmlu_clinical_knowledge/acc_stderr": 0.02512576648482785, "mmlu_college_medicine/acc": 0.24277456647398843, "mmlu_college_medicine/acc_stderr": 0.0326926380614177, "mmlu_global_facts/acc": 0.28, "mmlu_global_facts/acc_stderr": 0.04512608598542127, "mmlu_human_aging/acc": 0.31390134529147984, "mmlu_human_aging/acc_stderr": 0.031146796482972465, "mmlu_management/acc": 0.1941747572815534, "mmlu_management/acc_stderr": 0.03916667762822584, "mmlu_marketing/acc": 0.27350427350427353, "mmlu_marketing/acc_stderr": 0.02920254015343118, "mmlu_medical_genetics/acc": 0.25, "mmlu_medical_genetics/acc_stderr": 0.04351941398892446, "mmlu_miscellaneous/acc": 0.2503192848020434, "mmlu_miscellaneous/acc_stderr": 0.01549108895149459, "mmlu_nutrition/acc": 0.24836601307189543, "mmlu_nutrition/acc_stderr": 0.02473998135511359, "mmlu_professional_accounting/acc": 0.28368794326241137, "mmlu_professional_accounting/acc_stderr": 0.02689170942834396, "mmlu_professional_medicine/acc": 0.16911764705882354, "mmlu_professional_medicine/acc_stderr": 0.022770868010113004, "mmlu_virology/acc": 0.22289156626506024, "mmlu_virology/acc_stderr": 0.03240004825594687, "mmlu_social_sciences/acc": 0.2349691257718557, "mmlu_social_sciences/acc_stderr": 0.0076425238616364905, "mmlu_econometrics/acc": 0.2543859649122807, "mmlu_econometrics/acc_stderr": 0.040969851398436716, "mmlu_high_school_geography/acc": 0.25252525252525254, "mmlu_high_school_geography/acc_stderr": 0.030954055470365907, "mmlu_high_school_government_and_politics/acc": 0.2538860103626943, "mmlu_high_school_government_and_politics/acc_stderr": 0.0314102478056532, "mmlu_high_school_macroeconomics/acc": 0.21025641025641026, "mmlu_high_school_macroeconomics/acc_stderr": 0.020660597485026924, "mmlu_high_school_microeconomics/acc": 0.20168067226890757, "mmlu_high_school_microeconomics/acc_stderr": 0.026064313406304516, "mmlu_high_school_psychology/acc": 0.22935779816513763, "mmlu_high_school_psychology/acc_stderr": 0.018025349724618684, "mmlu_human_sexuality/acc": 0.22137404580152673, "mmlu_human_sexuality/acc_stderr": 0.03641297081313729, "mmlu_professional_psychology/acc": 0.2761437908496732, "mmlu_professional_psychology/acc_stderr": 0.018087276935663137, "mmlu_public_relations/acc": 0.17272727272727273, "mmlu_public_relations/acc_stderr": 0.03620691833929217, "mmlu_security_studies/acc": 0.22448979591836735, "mmlu_security_studies/acc_stderr": 0.02671143055553842, "mmlu_sociology/acc": 0.23383084577114427, "mmlu_sociology/acc_stderr": 0.029929415408348384, "mmlu_us_foreign_policy/acc": 0.21, "mmlu_us_foreign_policy/acc_stderr": 0.040936018074033256, "mmlu_stem/acc": 0.25531240088804313, "mmlu_stem/acc_stderr": 0.007764909603701747, "mmlu_abstract_algebra/acc": 0.24, "mmlu_abstract_algebra/acc_stderr": 0.04292346959909284, "mmlu_anatomy/acc": 0.3037037037037037, "mmlu_anatomy/acc_stderr": 0.03972552884785137, "mmlu_astronomy/acc": 0.27631578947368424, "mmlu_astronomy/acc_stderr": 0.03639057569952925, "mmlu_college_biology/acc": 0.25, "mmlu_college_biology/acc_stderr": 0.03621034121889507, "mmlu_college_chemistry/acc": 0.2, "mmlu_college_chemistry/acc_stderr": 0.04020151261036845, "mmlu_college_computer_science/acc": 0.24, "mmlu_college_computer_science/acc_stderr": 0.04292346959909284, "mmlu_college_mathematics/acc": 0.18, "mmlu_college_mathematics/acc_stderr": 0.038612291966536934, "mmlu_college_physics/acc": 0.22549019607843138, "mmlu_college_physics/acc_stderr": 0.041583075330832865, "mmlu_computer_security/acc": 0.29, "mmlu_computer_security/acc_stderr": 0.045604802157206845, "mmlu_conceptual_physics/acc": 0.19574468085106383, "mmlu_conceptual_physics/acc_stderr": 0.025937853139977148, "mmlu_electrical_engineering/acc": 0.2689655172413793, "mmlu_electrical_engineering/acc_stderr": 0.03695183311650232, "mmlu_elementary_mathematics/acc": 0.25925925925925924, "mmlu_elementary_mathematics/acc_stderr": 0.02256989707491842, "mmlu_high_school_biology/acc": 0.25806451612903225, "mmlu_high_school_biology/acc_stderr": 0.024892469172462857, "mmlu_high_school_chemistry/acc": 0.3054187192118227, "mmlu_high_school_chemistry/acc_stderr": 0.03240661565868408, "mmlu_high_school_computer_science/acc": 0.33, "mmlu_high_school_computer_science/acc_stderr": 0.04725815626252605, "mmlu_high_school_mathematics/acc": 0.26666666666666666, "mmlu_high_school_mathematics/acc_stderr": 0.026962424325073835, "mmlu_high_school_physics/acc": 0.24503311258278146, "mmlu_high_school_physics/acc_stderr": 0.035118075718047245, "mmlu_high_school_statistics/acc": 0.23148148148148148, "mmlu_high_school_statistics/acc_stderr": 0.028765111718046948, "mmlu_machine_learning/acc": 0.2767857142857143, "mmlu_machine_learning/acc_stderr": 0.04246624336697624, "hellaswag/acc": 0.4153555068711412, "hellaswag/acc_stderr": 0.004917761181740161, "hellaswag/acc_norm": 0.5370444134634534, "hellaswag/acc_norm_stderr": 0.004976067726432573, "gsm8k/exact_match,strict-match": 0.008339651250947688, "gsm8k/exact_match_stderr,strict-match": 0.0025049422268605395, "gsm8k/exact_match,flexible-extract": 0.01592115238817286, "gsm8k/exact_match_stderr,flexible-extract": 0.0034478192723890236, "arc_challenge/acc": 0.23378839590443687, "arc_challenge/acc_stderr": 0.012368225378507163, "arc_challenge/acc_norm": 0.295221843003413, "arc_challenge/acc_norm_stderr": 0.013329750293382316, "_timestamp": 1711093826.477364, "_runtime": 2727.445753097534, "_step": 2, "evaluation/eval_results": {"_type": "table-file", "sha256": "aceb251638b62234b60100043a3b36f8c00df19ecf09abaa9086d89719d34f94", "size": 6708, "path": "media/table/evaluation/eval_results_1_aceb251638b62234b601.table.json", "ncols": 7, "nrows": 80}, "evaluation/group_eval_results": {"_type": "table-file", "sha256": "5294d3b85d663a1a1968c8066e30d3d00c983cc200d058a25f82d93113f1274c", "size": 1498, "path": "media/table/evaluation/group_eval_results_2_5294d3b85d663a1a1968.table.json", "ncols": 7, "nrows": 18}, "_wandb": {"runtime": 2727}}