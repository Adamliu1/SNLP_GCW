{"winogrande/alias": "winogrande", "truthfulqa/alias": "truthfulqa", "truthfulqa_gen/alias": " - truthfulqa_gen", "truthfulqa_mc1/alias": " - truthfulqa_mc1", "truthfulqa_mc2/alias": " - truthfulqa_mc2", "toxigen/alias": "toxigen", "mmlu/alias": "mmlu", "mmlu_humanities/alias": " - humanities", "mmlu_formal_logic/alias": "  - formal_logic", "mmlu_high_school_european_history/alias": "  - high_school_european_history", "mmlu_high_school_us_history/alias": "  - high_school_us_history", "mmlu_high_school_world_history/alias": "  - high_school_world_history", "mmlu_international_law/alias": "  - international_law", "mmlu_jurisprudence/alias": "  - jurisprudence", "mmlu_logical_fallacies/alias": "  - logical_fallacies", "mmlu_moral_disputes/alias": "  - moral_disputes", "mmlu_moral_scenarios/alias": "  - moral_scenarios", "mmlu_philosophy/alias": "  - philosophy", "mmlu_prehistory/alias": "  - prehistory", "mmlu_professional_law/alias": "  - professional_law", "mmlu_world_religions/alias": "  - world_religions", "mmlu_other/alias": " - other", "mmlu_business_ethics/alias": "  - business_ethics", "mmlu_clinical_knowledge/alias": "  - clinical_knowledge", "mmlu_college_medicine/alias": "  - college_medicine", "mmlu_global_facts/alias": "  - global_facts", "mmlu_human_aging/alias": "  - human_aging", "mmlu_management/alias": "  - management", "mmlu_marketing/alias": "  - marketing", "mmlu_medical_genetics/alias": "  - medical_genetics", "mmlu_miscellaneous/alias": "  - miscellaneous", "mmlu_nutrition/alias": "  - nutrition", "mmlu_professional_accounting/alias": "  - professional_accounting", "mmlu_professional_medicine/alias": "  - professional_medicine", "mmlu_virology/alias": "  - virology", "mmlu_social_sciences/alias": " - social_sciences", "mmlu_econometrics/alias": "  - econometrics", "mmlu_high_school_geography/alias": "  - high_school_geography", "mmlu_high_school_government_and_politics/alias": "  - high_school_government_and_politics", "mmlu_high_school_macroeconomics/alias": "  - high_school_macroeconomics", "mmlu_high_school_microeconomics/alias": "  - high_school_microeconomics", "mmlu_high_school_psychology/alias": "  - high_school_psychology", "mmlu_human_sexuality/alias": "  - human_sexuality", "mmlu_professional_psychology/alias": "  - professional_psychology", "mmlu_public_relations/alias": "  - public_relations", "mmlu_security_studies/alias": "  - security_studies", "mmlu_sociology/alias": "  - sociology", "mmlu_us_foreign_policy/alias": "  - us_foreign_policy", "mmlu_stem/alias": " - stem", "mmlu_abstract_algebra/alias": "  - abstract_algebra", "mmlu_anatomy/alias": "  - anatomy", "mmlu_astronomy/alias": "  - astronomy", "mmlu_college_biology/alias": "  - college_biology", "mmlu_college_chemistry/alias": "  - college_chemistry", "mmlu_college_computer_science/alias": "  - college_computer_science", "mmlu_college_mathematics/alias": "  - college_mathematics", "mmlu_college_physics/alias": "  - college_physics", "mmlu_computer_security/alias": "  - computer_security", "mmlu_conceptual_physics/alias": "  - conceptual_physics", "mmlu_electrical_engineering/alias": "  - electrical_engineering", "mmlu_elementary_mathematics/alias": "  - elementary_mathematics", "mmlu_high_school_biology/alias": "  - high_school_biology", "mmlu_high_school_chemistry/alias": "  - high_school_chemistry", "mmlu_high_school_computer_science/alias": "  - high_school_computer_science", "mmlu_high_school_mathematics/alias": "  - high_school_mathematics", "mmlu_high_school_physics/alias": "  - high_school_physics", "mmlu_high_school_statistics/alias": "  - high_school_statistics", "mmlu_machine_learning/alias": "  - machine_learning", "hellaswag/alias": "hellaswag", "gsm8k/alias": "gsm8k", "arc_challenge/alias": "arc_challenge", "winogrande/acc": 0.5824782951854776, "winogrande/acc_stderr": 0.013859978264440251, "truthfulqa/bleu_max": 14.719146880903457, "truthfulqa/bleu_max_stderr": 0.5289410956306887, "truthfulqa/rouge1_diff": -7.826860132318145, "truthfulqa/rouge1_diff_stderr": 0.8148466908287811, "truthfulqa/acc": 0.342726257158398, "truthfulqa/acc_stderr": 0.010585870588106897, "truthfulqa/rouge2_diff": -7.566630212273278, "truthfulqa/rouge2_diff_stderr": 0.79252596242876, "truthfulqa/rouge2_max": 14.59844843788554, "truthfulqa/rouge2_max_stderr": 0.8159256391537836, "truthfulqa/rougeL_diff": -7.588929420718058, "truthfulqa/rougeL_diff_stderr": 0.8057153517732918, "truthfulqa/rouge1_acc": 0.2974296205630355, "truthfulqa/rouge1_acc_stderr": 0.016002651487360985, "truthfulqa/bleu_diff": -2.739727516251421, "truthfulqa/bleu_diff_stderr": 0.548482744758802, "truthfulqa/rouge1_max": 31.331086589776536, "truthfulqa/rouge1_max_stderr": 0.847391320000743, "truthfulqa/bleu_acc": 0.42105263157894735, "truthfulqa/bleu_acc_stderr": 0.01728393624813647, "truthfulqa/rouge2_acc": 0.1481028151774786, "truthfulqa/rouge2_acc_stderr": 0.012434552750319289, "truthfulqa/rougeL_max": 29.33928099938766, "truthfulqa/rougeL_max_stderr": 0.809810624339553, "truthfulqa/rougeL_acc": 0.2998776009791922, "truthfulqa/rougeL_acc_stderr": 0.016040352966713647, "truthfulqa_gen/bleu_max": 14.719146880903457, "truthfulqa_gen/bleu_max_stderr": 0.5289410956306887, "truthfulqa_gen/bleu_acc": 0.42105263157894735, "truthfulqa_gen/bleu_acc_stderr": 0.01728393624813647, "truthfulqa_gen/bleu_diff": -2.739727516251421, "truthfulqa_gen/bleu_diff_stderr": 0.548482744758802, "truthfulqa_gen/rouge1_max": 31.331086589776536, "truthfulqa_gen/rouge1_max_stderr": 0.8473913200007429, "truthfulqa_gen/rouge1_acc": 0.2974296205630355, "truthfulqa_gen/rouge1_acc_stderr": 0.016002651487360985, "truthfulqa_gen/rouge1_diff": -7.826860132318145, "truthfulqa_gen/rouge1_diff_stderr": 0.8148466908287811, "truthfulqa_gen/rouge2_max": 14.59844843788554, "truthfulqa_gen/rouge2_max_stderr": 0.8159256391537835, "truthfulqa_gen/rouge2_acc": 0.1481028151774786, "truthfulqa_gen/rouge2_acc_stderr": 0.012434552750319289, "truthfulqa_gen/rouge2_diff": -7.566630212273278, "truthfulqa_gen/rouge2_diff_stderr": 0.79252596242876, "truthfulqa_gen/rougeL_max": 29.33928099938766, "truthfulqa_gen/rougeL_max_stderr": 0.809810624339553, "truthfulqa_gen/rougeL_acc": 0.2998776009791922, "truthfulqa_gen/rougeL_acc_stderr": 0.016040352966713647, "truthfulqa_gen/rougeL_diff": -7.588929420718058, "truthfulqa_gen/rougeL_diff_stderr": 0.8057153517732919, "truthfulqa_mc1/acc": 0.2668298653610771, "truthfulqa_mc1/acc_stderr": 0.015483691939237256, "truthfulqa_mc2/acc": 0.41862264895571893, "truthfulqa_mc2/acc_stderr": 0.014439456650559462, "toxigen/acc": 0.548936170212766, "toxigen/acc_stderr": 0.016238545606460795, "toxigen/acc_norm": 0.4372340425531915, "toxigen/acc_norm_stderr": 0.016187809958489365, "mmlu/acc": 0.24540663723116365, "mmlu/acc_stderr": 0.003630182112389923, "mmlu_humanities/acc": 0.2461211477151966, "mmlu_humanities/acc_stderr": 0.0062777320176085364, "mmlu_formal_logic/acc": 0.20634920634920634, "mmlu_formal_logic/acc_stderr": 0.0361960452412425, "mmlu_high_school_european_history/acc": 0.23636363636363636, "mmlu_high_school_european_history/acc_stderr": 0.03317505930009179, "mmlu_high_school_us_history/acc": 0.21568627450980393, "mmlu_high_school_us_history/acc_stderr": 0.028867431449849313, "mmlu_high_school_world_history/acc": 0.20675105485232068, "mmlu_high_school_world_history/acc_stderr": 0.026361651668389087, "mmlu_international_law/acc": 0.2975206611570248, "mmlu_international_law/acc_stderr": 0.04173349148083498, "mmlu_jurisprudence/acc": 0.3055555555555556, "mmlu_jurisprudence/acc_stderr": 0.044531975073749834, "mmlu_logical_fallacies/acc": 0.27607361963190186, "mmlu_logical_fallacies/acc_stderr": 0.0351238528370505, "mmlu_moral_disputes/acc": 0.28901734104046245, "mmlu_moral_disputes/acc_stderr": 0.024405173935783238, "mmlu_moral_scenarios/acc": 0.24022346368715083, "mmlu_moral_scenarios/acc_stderr": 0.01428834380392532, "mmlu_philosophy/acc": 0.2282958199356913, "mmlu_philosophy/acc_stderr": 0.02383930331139822, "mmlu_prehistory/acc": 0.2839506172839506, "mmlu_prehistory/acc_stderr": 0.02508947852376513, "mmlu_professional_law/acc": 0.24119947848761408, "mmlu_professional_law/acc_stderr": 0.010926496102034952, "mmlu_world_religions/acc": 0.2222222222222222, "mmlu_world_religions/acc_stderr": 0.03188578017686398, "mmlu_other/acc": 0.24396523978113938, "mmlu_other/acc_stderr": 0.007708847941719533, "mmlu_business_ethics/acc": 0.22, "mmlu_business_ethics/acc_stderr": 0.041633319989322695, "mmlu_clinical_knowledge/acc": 0.2490566037735849, "mmlu_clinical_knowledge/acc_stderr": 0.026616482980501708, "mmlu_college_medicine/acc": 0.24855491329479767, "mmlu_college_medicine/acc_stderr": 0.03295304696818318, "mmlu_global_facts/acc": 0.24, "mmlu_global_facts/acc_stderr": 0.04292346959909283, "mmlu_human_aging/acc": 0.30493273542600896, "mmlu_human_aging/acc_stderr": 0.030898610882477515, "mmlu_management/acc": 0.23300970873786409, "mmlu_management/acc_stderr": 0.04185832598928315, "mmlu_marketing/acc": 0.2264957264957265, "mmlu_marketing/acc_stderr": 0.027421007295392943, "mmlu_medical_genetics/acc": 0.21, "mmlu_medical_genetics/acc_stderr": 0.040936018074033256, "mmlu_miscellaneous/acc": 0.2413793103448276, "mmlu_miscellaneous/acc_stderr": 0.015302380123542094, "mmlu_nutrition/acc": 0.20915032679738563, "mmlu_nutrition/acc_stderr": 0.023287685312334806, "mmlu_professional_accounting/acc": 0.2624113475177305, "mmlu_professional_accounting/acc_stderr": 0.026244920349843007, "mmlu_professional_medicine/acc": 0.26838235294117646, "mmlu_professional_medicine/acc_stderr": 0.026917481224377215, "mmlu_virology/acc": 0.22289156626506024, "mmlu_virology/acc_stderr": 0.03240004825594687, "mmlu_social_sciences/acc": 0.2349691257718557, "mmlu_social_sciences/acc_stderr": 0.0076384116888532595, "mmlu_econometrics/acc": 0.24561403508771928, "mmlu_econometrics/acc_stderr": 0.04049339297748142, "mmlu_high_school_geography/acc": 0.24242424242424243, "mmlu_high_school_geography/acc_stderr": 0.030532892233932026, "mmlu_high_school_government_and_politics/acc": 0.20725388601036268, "mmlu_high_school_government_and_politics/acc_stderr": 0.029252823291803644, "mmlu_high_school_macroeconomics/acc": 0.21025641025641026, "mmlu_high_school_macroeconomics/acc_stderr": 0.020660597485026924, "mmlu_high_school_microeconomics/acc": 0.226890756302521, "mmlu_high_school_microeconomics/acc_stderr": 0.027205371538279483, "mmlu_high_school_psychology/acc": 0.24770642201834864, "mmlu_high_school_psychology/acc_stderr": 0.018508143602547822, "mmlu_human_sexuality/acc": 0.15267175572519084, "mmlu_human_sexuality/acc_stderr": 0.0315452167200547, "mmlu_professional_psychology/acc": 0.2581699346405229, "mmlu_professional_psychology/acc_stderr": 0.01770453165325007, "mmlu_public_relations/acc": 0.16363636363636364, "mmlu_public_relations/acc_stderr": 0.035434330542986774, "mmlu_security_studies/acc": 0.2816326530612245, "mmlu_security_studies/acc_stderr": 0.028795185574291282, "mmlu_sociology/acc": 0.24378109452736318, "mmlu_sociology/acc_stderr": 0.030360490154014652, "mmlu_us_foreign_policy/acc": 0.22, "mmlu_us_foreign_policy/acc_stderr": 0.041633319989322695, "mmlu_stem/acc": 0.2559467174119886, "mmlu_stem/acc_stderr": 0.00776846845753994, "mmlu_abstract_algebra/acc": 0.31, "mmlu_abstract_algebra/acc_stderr": 0.04648231987117316, "mmlu_anatomy/acc": 0.24444444444444444, "mmlu_anatomy/acc_stderr": 0.037125378336148665, "mmlu_astronomy/acc": 0.18421052631578946, "mmlu_astronomy/acc_stderr": 0.0315469804508223, "mmlu_college_biology/acc": 0.2152777777777778, "mmlu_college_biology/acc_stderr": 0.03437079344106133, "mmlu_college_chemistry/acc": 0.16, "mmlu_college_chemistry/acc_stderr": 0.036845294917747094, "mmlu_college_computer_science/acc": 0.25, "mmlu_college_computer_science/acc_stderr": 0.04351941398892446, "mmlu_college_mathematics/acc": 0.29, "mmlu_college_mathematics/acc_stderr": 0.045604802157206845, "mmlu_college_physics/acc": 0.22549019607843138, "mmlu_college_physics/acc_stderr": 0.04158307533083286, "mmlu_computer_security/acc": 0.21, "mmlu_computer_security/acc_stderr": 0.040936018074033256, "mmlu_conceptual_physics/acc": 0.2553191489361702, "mmlu_conceptual_physics/acc_stderr": 0.028504856470514185, "mmlu_electrical_engineering/acc": 0.2827586206896552, "mmlu_electrical_engineering/acc_stderr": 0.03752833958003336, "mmlu_elementary_mathematics/acc": 0.2777777777777778, "mmlu_elementary_mathematics/acc_stderr": 0.0230681888482611, "mmlu_high_school_biology/acc": 0.24516129032258063, "mmlu_high_school_biology/acc_stderr": 0.024472243840895552, "mmlu_high_school_chemistry/acc": 0.3054187192118227, "mmlu_high_school_chemistry/acc_stderr": 0.032406615658684086, "mmlu_high_school_computer_science/acc": 0.3, "mmlu_high_school_computer_science/acc_stderr": 0.04605661864718381, "mmlu_high_school_mathematics/acc": 0.23703703703703705, "mmlu_high_school_mathematics/acc_stderr": 0.025928876132766118, "mmlu_high_school_physics/acc": 0.25165562913907286, "mmlu_high_school_physics/acc_stderr": 0.035433042343899844, "mmlu_high_school_statistics/acc": 0.2916666666666667, "mmlu_high_school_statistics/acc_stderr": 0.030998666304560534, "mmlu_machine_learning/acc": 0.2767857142857143, "mmlu_machine_learning/acc_stderr": 0.042466243366976256, "hellaswag/acc": 0.3316072495518821, "hellaswag/acc_stderr": 0.004698285350019211, "hellaswag/acc_norm": 0.4018123879705238, "hellaswag/acc_norm_stderr": 0.00489262449093724, "gsm8k/exact_match,strict-match": 0.015163002274450341, "gsm8k/exact_match_stderr,strict-match": 0.003366022949726344, "gsm8k/exact_match,flexible-extract": 0.01819560272934041, "gsm8k/exact_match_stderr,flexible-extract": 0.003681611894073871, "arc_challenge/acc": 0.21331058020477817, "arc_challenge/acc_stderr": 0.011970971742326334, "arc_challenge/acc_norm": 0.2721843003412969, "arc_challenge/acc_norm_stderr": 0.013006600406423704, "_timestamp": 1711096147.3268757, "_runtime": 2307.8838217258453, "_step": 2, "evaluation/eval_results": {"_type": "table-file", "sha256": "516837673ebfb586131bdd3bbe05e60420924e7a329c1c5ae67cbcde0202972e", "size": 6732, "path": "media/table/evaluation/eval_results_1_516837673ebfb586131b.table.json", "ncols": 7, "nrows": 80}, "evaluation/group_eval_results": {"_type": "table-file", "sha256": "ad6509e6e9b780e51f671faa0a85ec95352fda0e289ff65644d8b99dc8a9d901", "size": 1496, "path": "media/table/evaluation/group_eval_results_2_ad6509e6e9b780e51f67.table.json", "ncols": 7, "nrows": 18}, "_wandb": {"runtime": 2308}}