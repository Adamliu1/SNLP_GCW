2024-06-09 12:11:32,081 INFO    MainThread:45321 [wandb_setup.py:_flush():76] Current SDK version is 0.17.1
2024-06-09 12:11:32,081 INFO    MainThread:45321 [wandb_setup.py:_flush():76] Configure stats pid to 45321
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_setup.py:_flush():76] Loading settings from /home/aszablew/.config/wandb/settings
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_setup.py:_flush():76] Loading settings from /SAN/intelsys/llm/aszablew/snlp/SNLP_GCW/eval_framework_tasks/wandb/settings
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2024-06-09 12:11:32,082 WARNING MainThread:45321 [wandb_setup.py:_flush():76] Could not find program at -m lm_eval.__main__
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': None, 'program': '-m lm_eval.__main__'}
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_init.py:_log_setup():520] Logging user logs to /scratch0/aszablew/phi-1_5-eval1/wandb/wandb/offline-run-20240609_121132-jmhotau7/logs/debug.log
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_init.py:_log_setup():521] Logging internal logs to /scratch0/aszablew/phi-1_5-eval1/wandb/wandb/offline-run-20240609_121132-jmhotau7/logs/debug-internal.log
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_init.py:init():560] calling init triggers
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_init.py:init():567] wandb.init called with sweep_config: {}
config: {}
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_init.py:init():610] starting backend
2024-06-09 12:11:32,082 INFO    MainThread:45321 [wandb_init.py:init():614] setting up manager
2024-06-09 12:11:32,085 INFO    MainThread:45321 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-06-09 12:11:32,089 INFO    MainThread:45321 [wandb_init.py:init():622] backend started and connected
2024-06-09 12:11:32,099 INFO    MainThread:45321 [wandb_init.py:init():711] updated telemetry
2024-06-09 12:11:32,134 INFO    MainThread:45321 [wandb_init.py:init():744] communicating run to backend with 90.0 second timeout
2024-06-09 12:11:32,140 INFO    MainThread:45321 [wandb_init.py:init():795] starting run threads in backend
2024-06-09 12:11:32,465 INFO    MainThread:45321 [wandb_run.py:_console_start():2380] atexit reg
2024-06-09 12:11:32,465 INFO    MainThread:45321 [wandb_run.py:_redirect():2235] redirect: wrap_raw
2024-06-09 12:11:32,465 INFO    MainThread:45321 [wandb_run.py:_redirect():2300] Wrapping output streams.
2024-06-09 12:11:32,465 INFO    MainThread:45321 [wandb_run.py:_redirect():2325] Redirects installed.
2024-06-09 12:11:32,468 INFO    MainThread:45321 [wandb_init.py:init():838] run started, returning control to user process
2024-06-09 13:46:22,949 INFO    MainThread:45321 [wandb_run.py:_config_callback():1382] config_cb None None {'task_configs': {'arc_challenge': {'task': 'arc_challenge', 'group': ['ai2_arc'], 'dataset_path': 'allenai/ai2_arc', 'dataset_name': 'ARC-Challenge', 'training_split': 'train', 'validation_split': 'validation', 'test_split': 'test', 'doc_to_text': 'Question: {{question}}\nAnswer:', 'doc_to_target': '{{choices.label.index(answerKey)}}', 'doc_to_choice': '{{choices.text}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'Question: {{question}}\nAnswer:', 'metadata': {'version': 1.0}}, 'french_bench_arc_challenge': {'task': 'french_bench_arc_challenge', 'group': ['french_bench', 'french_bench_mc'], 'dataset_path': 'manu/french_bench_arc_challenge', 'training_split': 'train', 'validation_split': 'validation', 'test_split': 'test', 'doc_to_text': 'Question: {{question}}\nRéponse:', 'doc_to_target': "{{['A', 'B', 'C', 'D'].index(answerKey)}}", 'doc_to_choice': '{{choices}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'Question: {{question}}\nRéponse:'}, 'french_bench_boolqa': {'task': 'french_bench_boolqa', 'group': ['french_bench', 'french_bench_extra'], 'dataset_path': 'manu/french_boolq', 'validation_split': 'valid', 'test_split': 'test', 'fewshot_split': 'valid', 'doc_to_text': '\nContexte: {{passage}}\n\nQuestion: {{question}}\n', 'doc_to_target': '{{[1, 0].index(label)}}', 'doc_to_choice': ['Oui', 'Non'], 'description': "D'après l'information dans le contexte donné, quelle est la réponse à la question ?", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'passage'}, 'french_bench_fquadv2': {'task': 'french_bench_fquadv2', 'group': ['french_bench', 'french_bench_extra'], 'dataset_path': 'manu/fquad2_test', 'validation_split': 'valid', 'test_split': 'test', 'fewshot_split': 'valid', 'doc_to_text': '\nContexte: {{context}}\n\nQuestion: {{question}}\n\nRéponse:', 'doc_to_target': "{% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %}", 'description': "D'après l'information dans le contexte donné, donne la réponse à la question en citant quelques mots du contexte. Si il est impossible de répondre avec les informations du contexte, répond 'Impossible'.", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'def exact(predictions, references):\n    return int(normalize_answer(references[0]) == normalize_answer(predictions[0]))\n', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'def f1(predictions, references):\n    gold_toks = get_tokens(references[0])\n    pred_toks = get_tokens(predictions[0])\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n']}, 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'context'}, 'french_bench_fquadv2_bool': {'task': 'french_bench_fquadv2_bool', 'group': ['french_bench', 'french_bench_extra'], 'dataset_path': 'manu/fquad2_test', 'validation_split': 'valid', 'test_split': 'test', 'fewshot_split': 'valid', 'doc_to_text': "\nContexte: {{context}}\n\nQuestion: {{question}}\n\nD'après l'information présente dans le contexte, répondre à la question est:\nA. Possible \nB. Impossible\n\nRéponse:", 'doc_to_target': '{{[False, True].index(is_impossible)}}', 'doc_to_choice': ['A', 'B'], 'description': "D'après l'information présente dans le contexte, est il possible de répondre à la question ?", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'context'}, 'french_bench_fquadv2_genq': {'task': 'french_bench_fquadv2_genq', 'group': ['french_bench', 'french_bench_gen'], 'dataset_path': 'manu/fquad2_test', 'validation_split': 'valid_hasAns', 'test_split': 'test_hasAns', 'fewshot_split': 'valid_hasAns', 'doc_to_text': "\nContexte: {{context}}\n\nRéponse: {% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %}\n\nQuestion:", 'doc_to_target': '{{question}}', 'description': "D'après l'information dans le contexte donné, quelle question a été posée pour obtenir la réponse donnée ?", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'def rouge1(items):\n    """\n    # passthrough for efficiency\n    """\n    return items\n', 'higher_is_better': True, 'aggregation': 'def rouge1_agg(items):\n    """\n    Higher is better\n    """\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    rouge_scorer = evaluate.load("rouge")\n    return rouge_scorer.compute(predictions=preds, references=refs)["rouge1"]\n'}, {'metric': 'def f1(predictions, references):\n    gold_toks = get_tokens(references[0])\n    pred_toks = get_tokens(predictions[0])\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n']}, 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'question'}, 'french_bench_fquadv2_hasAns': {'task': 'french_bench_fquadv2_hasAns', 'group': ['french_bench', 'french_bench_gen'], 'dataset_path': 'manu/fquad2_test', 'validation_split': 'valid_hasAns', 'test_split': 'test_hasAns', 'fewshot_split': 'valid_hasAns', 'doc_to_text': '\nContexte: {{context}}\n\nQuestion: {{question}}\n\nRéponse:', 'doc_to_target': "{% if answers.text| length > 0 %}{{answers.text[0]}}{% else %}{{['Impossible']}}{% endif %}", 'description': "D'après l'information dans le contexte donné, donne la réponse à la question en citant quelques mots du contexte. Si il est impossible de répondre avec les informations du contexte, répond 'Impossible'.", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'def exact(predictions, references):\n    return int(normalize_answer(references[0]) == normalize_answer(predictions[0]))\n', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'def f1(predictions, references):\n    gold_toks = get_tokens(references[0])\n    pred_toks = get_tokens(predictions[0])\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'def rouge1(items):\n    """\n    # passthrough for efficiency\n    """\n    return items\n', 'higher_is_better': True, 'aggregation': 'def rouge1_agg(items):\n    """\n    Higher is better\n    """\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    rouge_scorer = evaluate.load("rouge")\n    return rouge_scorer.compute(predictions=preds, references=refs)["rouge1"]\n'}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n']}, 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'context'}, 'french_bench_grammar': {'task': 'french_bench_grammar', 'group': ['french_bench', 'french_bench_mc'], 'dataset_path': 'manu/french-bench-grammar-vocab-reading', 'validation_split': 'Grammar', 'test_split': 'Grammar', 'fewshot_split': 'Grammar', 'doc_to_text': 'La phrase suivante est correcte grammaticalement:\n', 'doc_to_target': '{{["answerA", "answerB", "answerC", "answerD"].index("answer" + answer)}}', 'doc_to_choice': "{{[question.replace('<...>', answerA), question.replace('<...>', answerB), question.replace('<...>', answerC), question.replace('<...>', answerD)]}}", 'description': 'Répond au mieux en complétant la question avec une des réponses proposées.', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False}, 'french_bench_hellaswag': {'task': 'french_bench_hellaswag', 'group': ['french_bench', 'french_bench_mc'], 'dataset_path': 'manu/french_bench_hellaswag', 'training_split': 'validation', 'validation_split': 'validation', 'process_docs': 'def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc["ctx_a"] + " " + doc["ctx_b"].capitalize()\n        out_doc = {\n            "query": preprocess(doc["activity_label"] + ": " + ctx),\n            "choices": [preprocess(ending) for ending in doc["endings"]],\n            "gold": int(doc["label"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n', 'doc_to_text': '{{query}}', 'doc_to_target': '{{label}}', 'doc_to_choice': '{{choices}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False}, 'french_bench_multifquad': {'task': 'french_bench_multifquad', 'group': ['french_bench', 'french_bench_gen'], 'dataset_path': 'manu/multifquad_test', 'validation_split': 'valid', 'test_split': 'test', 'fewshot_split': 'valid', 'doc_to_text': '\nContexte: {{context}}\n\nQuestion: {{question}}\n\nRéponse:', 'doc_to_target': "{{', '.join(answers.text)}}", 'description': "D'après l'information dans le contexte donné, donne la réponse à la question en citant quelques extraits du contexte.", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'def exact(predictions, references):\n    return int(normalize_answer(references[0]) == normalize_answer(predictions[0]))\n', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'def f1(predictions, references):\n    gold_toks = get_tokens(references[0])\n    pred_toks = get_tokens(predictions[0])\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'def rouge1(items):\n    """\n    # passthrough for efficiency\n    """\n    return items\n', 'higher_is_better': True, 'aggregation': 'def rouge1_agg(items):\n    """\n    Higher is better\n    """\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    rouge_scorer = evaluate.load("rouge")\n    return rouge_scorer.compute(predictions=preds, references=refs)["rouge1"]\n'}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n']}, 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'context'}, 'french_bench_orangesum_abstract': {'task': 'french_bench_orangesum_abstract', 'group': ['french_bench', 'french_bench_gen'], 'dataset_path': 'orange_sum', 'dataset_name': 'abstract', 'validation_split': 'validation', 'test_split': 'test', 'fewshot_split': 'validation', 'doc_to_text': '\nArticle: {{text}}\n\nRésumé:', 'doc_to_target': '{{summary}}', 'description': "Résume l'article en une phrase.", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'def rouge1(items):\n    """\n    # passthrough for efficiency\n    """\n    return items\n', 'higher_is_better': True, 'aggregation': 'def rouge1_agg(items):\n    """\n    Higher is better\n    """\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    rouge_scorer = evaluate.load("rouge")\n    return rouge_scorer.compute(predictions=preds, references=refs)["rouge1"]\n'}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n']}, 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'summary'}, 'french_bench_orangesum_title': {'task': 'french_bench_orangesum_title', 'group': ['french_bench', 'french_bench_extra'], 'dataset_path': 'orange_sum', 'dataset_name': 'title', 'validation_split': 'validation', 'test_split': 'test', 'fewshot_split': 'validation', 'doc_to_text': '\nArticle: {{text}}\n\nTitre:', 'doc_to_target': '{{summary}}', 'description': "Trouve le titre de l'article.", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'def rouge1(items):\n    """\n    # passthrough for efficiency\n    """\n    return items\n', 'higher_is_better': True, 'aggregation': 'def rouge1_agg(items):\n    """\n    Higher is better\n    """\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    rouge_scorer = evaluate.load("rouge")\n    return rouge_scorer.compute(predictions=preds, references=refs)["rouge1"]\n'}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n']}, 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'summary'}, 'french_bench_reading_comp': {'task': 'french_bench_reading_comp', 'group': ['french_bench', 'french_bench_extra'], 'dataset_path': 'manu/french-bench-grammar-vocab-reading', 'validation_split': 'Reading', 'test_split': 'Reading', 'fewshot_split': 'Reading', 'doc_to_text': 'Context: {{context}}\n\n', 'doc_to_target': '{{["answerA", "answerB", "answerC", "answerD"].index("answer" + answer)}}', 'doc_to_choice': "{{[question.replace('<...>', answerA) if '<...>' in question else question + ' ' +answerA, question.replace('<...>', answerB) if '<...>' in question else question + ' ' + answerB, question.replace('<...>', answerC) if '<...>' in question else question + ' ' + answerC, question.replace('<...>', answerD) if '<...>' in question else question + ' ' + answerD]}}", 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False}, 'french_bench_topic_based_nli': {'task': 'french_bench_topic_based_nli', 'group': ['french_bench', 'french_bench_extra'], 'dataset_path': 'manu/topic_based_nli_test', 'validation_split': 'valid', 'test_split': 'test', 'fewshot_split': 'valid', 'doc_to_text': '\nAvis Client: {{text}}\n\nA propos du thème "{{topic}}", l\'avis client est', 'doc_to_target': "{{['positif', 'negatif', 'neutre'].index(polarity)}}", 'doc_to_choice': ['positif', 'négatif', 'neutre'], 'description': "A propos du thème spécifié, l'avis client est il positif, négatif, ou neutre ?", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'texte'}, 'french_bench_trivia': {'task': 'french_bench_trivia', 'group': ['french_bench', 'french_bench_gen'], 'dataset_path': 'manu/french-trivia', 'validation_split': 'train', 'test_split': 'train', 'fewshot_split': 'train', 'doc_to_text': '{{Question}}\nAnswer:', 'doc_to_target': '{{Answer}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'def exact(predictions, references):\n    return int(normalize_answer(references[0]) == normalize_answer(predictions[0]))\n', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'def f1(predictions, references):\n    gold_toks = get_tokens(references[0])\n    pred_toks = get_tokens(predictions[0])\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'def rouge1(items):\n    """\n    # passthrough for efficiency\n    """\n    return items\n', 'higher_is_better': True, 'aggregation': 'def rouge1_agg(items):\n    """\n    Higher is better\n    """\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    rouge_scorer = evaluate.load("rouge")\n    return rouge_scorer.compute(predictions=preds, references=refs)["rouge1"]\n'}, {'metric': 'def is_included(items):\n    """\n    # passthrough for efficiency\n    """\n    if items[0] in items[1]:\n        return True\n    return False\n', 'higher_is_better': True, 'aggregation': 'mean'}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n']}, 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'Question'}, 'french_bench_vocab': {'task': 'french_bench_vocab', 'group': ['french_bench', 'french_bench_mc'], 'dataset_path': 'manu/french-bench-grammar-vocab-reading', 'validation_split': 'Vocabulary', 'test_split': 'Vocabulary', 'fewshot_split': 'Vocabulary', 'doc_to_text': 'La phrase suivante est logique sémantiquement:\n', 'doc_to_target': '{{["answerA", "answerB", "answerC", "answerD"].index("answer" + answer)}}', 'doc_to_choice': "{{[question.replace('<...>', answerA), question.replace('<...>', answerB), question.replace('<...>', answerC), question.replace('<...>', answerD)]}}", 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False}, 'french_bench_xnli': {'task': 'french_bench_xnli', 'group': ['french_bench', 'french_bench_extra'], 'dataset_path': 'xnli', 'dataset_name': 'fr', 'validation_split': 'validation', 'test_split': 'test', 'fewshot_split': 'validation', 'doc_to_text': "\nPrémisse: {{premise}}\n\nHypothèse: {{hypothesis}}\n\nLa prémisse et l'hypothèse sont", 'doc_to_target': 'label', 'doc_to_choice': "{{['en accord', 'neutres entre elles', 'en contradiction']}}", 'description': "La prémisse et l'hypothèse sont elles en accord, neutres en elles, ou en contradiction ?", 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False}, 'gsm8k': {'task': 'gsm8k', 'group': ['math_word_problems'], 'dataset_path': 'gsm8k', 'dataset_name': 'main', 'training_split': 'train', 'test_split': 'test', 'fewshot_split': 'train', 'doc_to_text': 'Question: {{question}}\nAnswer:', 'doc_to_target': '{{answer}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 5, 'metric_list': [{'metric': 'exact_match', 'aggregation': 'mean', 'higher_is_better': True, 'ignore_case': True, 'ignore_punctuation': False, 'regexes_to_ignore': [',', '\\$', '(?s).*#### ', '\\.$']}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}, 'repeats': 1, 'filter_list': [{'name': 'strict-match', 'filter': [{'function': 'regex', 'regex_pattern': '#### (\\-?[0-9\\.\\,]+)'}, {'function': 'take_first'}]}, {'name': 'flexible-extract', 'filter': [{'function': 'regex', 'group_select': -1, 'regex_pattern': '(-?[$0-9.,]{2,})|(-?[0-9]+)'}, {'function': 'take_first'}]}], 'should_decontaminate': False, 'metadata': {'version': 3.0}}, 'hellaswag': {'task': 'hellaswag', 'group': ['multiple_choice'], 'dataset_path': 'hellaswag', 'training_split': 'train', 'validation_split': 'validation', 'process_docs': 'def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc["ctx_a"] + " " + doc["ctx_b"].capitalize()\n        out_doc = {\n            "query": preprocess(doc["activity_label"] + ": " + ctx),\n            "choices": [preprocess(ending) for ending in doc["endings"]],\n            "gold": int(doc["label"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n', 'doc_to_text': '{{query}}', 'doc_to_target': '{{label}}', 'doc_to_choice': 'choices', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 1.0}}, 'logiqa': {'task': 'logiqa', 'dataset_path': 'EleutherAI/logiqa', 'dataset_name': 'logiqa', 'dataset_kwargs': {'trust_remote_code': True}, 'training_split': 'train', 'validation_split': 'validation', 'test_split': 'test', 'doc_to_text': 'def doc_to_text(doc) -> str:\n    """\n    Passage: <passage>\n    Question: <question>\n    Choices:\n    A. <choice1>\n    B. <choice2>\n    C. <choice3>\n    D. <choice4>\n    Answer:\n    """\n    choices = ["a", "b", "c", "d"]\n    prompt = "Passage: " + doc["context"] + "\\n"\n    prompt += "Question: " + doc["question"] + "\\nChoices:\\n"\n    for choice, option in zip(choices, doc["options"]):\n        prompt += f"{choice.upper()}. {option}\\n"\n    prompt += "Answer:"\n    return prompt\n', 'doc_to_target': 'def doc_to_target(doc) -> int:\n    choices = ["a", "b", "c", "d"]\n    return choices.index(doc["label"].strip())\n', 'doc_to_choice': '{{options}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': '{{context}}', 'metadata': {'version': 1.0}}, 'mmlu_abstract_algebra': {'task': 'mmlu_abstract_algebra', 'task_alias': 'abstract_algebra', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'abstract_algebra', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about abstract algebra.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_anatomy': {'task': 'mmlu_anatomy', 'task_alias': 'anatomy', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'anatomy', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about anatomy.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_astronomy': {'task': 'mmlu_astronomy', 'task_alias': 'astronomy', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'astronomy', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about astronomy.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_business_ethics': {'task': 'mmlu_business_ethics', 'task_alias': 'business_ethics', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'business_ethics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about business ethics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_clinical_knowledge': {'task': 'mmlu_clinical_knowledge', 'task_alias': 'clinical_knowledge', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'clinical_knowledge', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about clinical knowledge.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_biology': {'task': 'mmlu_college_biology', 'task_alias': 'college_biology', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_biology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college biology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_chemistry': {'task': 'mmlu_college_chemistry', 'task_alias': 'college_chemistry', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_chemistry', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college chemistry.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_computer_science': {'task': 'mmlu_college_computer_science', 'task_alias': 'college_computer_science', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_computer_science', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college computer science.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_mathematics': {'task': 'mmlu_college_mathematics', 'task_alias': 'college_mathematics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_mathematics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college mathematics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_medicine': {'task': 'mmlu_college_medicine', 'task_alias': 'college_medicine', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_medicine', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college medicine.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_college_physics': {'task': 'mmlu_college_physics', 'task_alias': 'college_physics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'college_physics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about college physics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_computer_security': {'task': 'mmlu_computer_security', 'task_alias': 'computer_security', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'computer_security', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about computer security.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_conceptual_physics': {'task': 'mmlu_conceptual_physics', 'task_alias': 'conceptual_physics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'conceptual_physics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about conceptual physics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_econometrics': {'task': 'mmlu_econometrics', 'task_alias': 'econometrics', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'econometrics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about econometrics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_electrical_engineering': {'task': 'mmlu_electrical_engineering', 'task_alias': 'electrical_engineering', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'electrical_engineering', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about electrical engineering.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_elementary_mathematics': {'task': 'mmlu_elementary_mathematics', 'task_alias': 'elementary_mathematics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'elementary_mathematics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about elementary mathematics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_formal_logic': {'task': 'mmlu_formal_logic', 'task_alias': 'formal_logic', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'formal_logic', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about formal logic.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_global_facts': {'task': 'mmlu_global_facts', 'task_alias': 'global_facts', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'global_facts', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about global facts.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_biology': {'task': 'mmlu_high_school_biology', 'task_alias': 'high_school_biology', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_biology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school biology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_chemistry': {'task': 'mmlu_high_school_chemistry', 'task_alias': 'high_school_chemistry', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_chemistry', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school chemistry.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_computer_science': {'task': 'mmlu_high_school_computer_science', 'task_alias': 'high_school_computer_science', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_computer_science', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school computer science.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_european_history': {'task': 'mmlu_high_school_european_history', 'task_alias': 'high_school_european_history', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_european_history', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school european history.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_geography': {'task': 'mmlu_high_school_geography', 'task_alias': 'high_school_geography', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_geography', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school geography.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_government_and_politics': {'task': 'mmlu_high_school_government_and_politics', 'task_alias': 'high_school_government_and_politics', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_government_and_politics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school government and politics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_macroeconomics': {'task': 'mmlu_high_school_macroeconomics', 'task_alias': 'high_school_macroeconomics', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_macroeconomics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school macroeconomics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_mathematics': {'task': 'mmlu_high_school_mathematics', 'task_alias': 'high_school_mathematics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_mathematics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school mathematics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_microeconomics': {'task': 'mmlu_high_school_microeconomics', 'task_alias': 'high_school_microeconomics', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_microeconomics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school microeconomics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_physics': {'task': 'mmlu_high_school_physics', 'task_alias': 'high_school_physics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_physics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school physics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_psychology': {'task': 'mmlu_high_school_psychology', 'task_alias': 'high_school_psychology', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_psychology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school psychology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_statistics': {'task': 'mmlu_high_school_statistics', 'task_alias': 'high_school_statistics', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_statistics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school statistics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_us_history': {'task': 'mmlu_high_school_us_history', 'task_alias': 'high_school_us_history', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_us_history', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school us history.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_high_school_world_history': {'task': 'mmlu_high_school_world_history', 'task_alias': 'high_school_world_history', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'high_school_world_history', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about high school world history.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_human_aging': {'task': 'mmlu_human_aging', 'task_alias': 'human_aging', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'human_aging', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about human aging.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_human_sexuality': {'task': 'mmlu_human_sexuality', 'task_alias': 'human_sexuality', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'human_sexuality', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about human sexuality.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_international_law': {'task': 'mmlu_international_law', 'task_alias': 'international_law', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'international_law', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about international law.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_jurisprudence': {'task': 'mmlu_jurisprudence', 'task_alias': 'jurisprudence', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'jurisprudence', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about jurisprudence.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_logical_fallacies': {'task': 'mmlu_logical_fallacies', 'task_alias': 'logical_fallacies', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'logical_fallacies', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about logical fallacies.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_machine_learning': {'task': 'mmlu_machine_learning', 'task_alias': 'machine_learning', 'group': 'mmlu_stem', 'group_alias': 'stem', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'machine_learning', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about machine learning.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_management': {'task': 'mmlu_management', 'task_alias': 'management', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'management', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about management.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_marketing': {'task': 'mmlu_marketing', 'task_alias': 'marketing', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'marketing', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about marketing.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_medical_genetics': {'task': 'mmlu_medical_genetics', 'task_alias': 'medical_genetics', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'medical_genetics', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about medical genetics.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_miscellaneous': {'task': 'mmlu_miscellaneous', 'task_alias': 'miscellaneous', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'miscellaneous', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about miscellaneous.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_moral_disputes': {'task': 'mmlu_moral_disputes', 'task_alias': 'moral_disputes', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'moral_disputes', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about moral disputes.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_moral_scenarios': {'task': 'mmlu_moral_scenarios', 'task_alias': 'moral_scenarios', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'moral_scenarios', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about moral scenarios.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_nutrition': {'task': 'mmlu_nutrition', 'task_alias': 'nutrition', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'nutrition', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about nutrition.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_philosophy': {'task': 'mmlu_philosophy', 'task_alias': 'philosophy', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'philosophy', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about philosophy.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_prehistory': {'task': 'mmlu_prehistory', 'task_alias': 'prehistory', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'prehistory', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about prehistory.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_professional_accounting': {'task': 'mmlu_professional_accounting', 'task_alias': 'professional_accounting', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'professional_accounting', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about professional accounting.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_professional_law': {'task': 'mmlu_professional_law', 'task_alias': 'professional_law', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'professional_law', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about professional law.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_professional_medicine': {'task': 'mmlu_professional_medicine', 'task_alias': 'professional_medicine', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'professional_medicine', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about professional medicine.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_professional_psychology': {'task': 'mmlu_professional_psychology', 'task_alias': 'professional_psychology', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'professional_psychology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about professional psychology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_public_relations': {'task': 'mmlu_public_relations', 'task_alias': 'public_relations', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'public_relations', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about public relations.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_security_studies': {'task': 'mmlu_security_studies', 'task_alias': 'security_studies', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'security_studies', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about security studies.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_sociology': {'task': 'mmlu_sociology', 'task_alias': 'sociology', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'sociology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about sociology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_us_foreign_policy': {'task': 'mmlu_us_foreign_policy', 'task_alias': 'us_foreign_policy', 'group': 'mmlu_social_sciences', 'group_alias': 'social_sciences', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'us_foreign_policy', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about us foreign policy.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_virology': {'task': 'mmlu_virology', 'task_alias': 'virology', 'group': 'mmlu_other', 'group_alias': 'other', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'virology', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about virology.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'mmlu_world_religions': {'task': 'mmlu_world_religions', 'task_alias': 'world_religions', 'group': 'mmlu_humanities', 'group_alias': 'humanities', 'dataset_path': 'hails/mmlu_no_train', 'dataset_name': 'world_religions', 'test_split': 'test', 'fewshot_split': 'dev', 'doc_to_text': '{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:', 'doc_to_target': 'answer', 'doc_to_choice': ['A', 'B', 'C', 'D'], 'description': 'The following are multiple choice questions (with answers) about world religions.\n\n', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'fewshot_config': {'sampler': 'first_n'}, 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 0.0}}, 'piqa': {'task': 'piqa', 'dataset_path': 'piqa', 'training_split': 'train', 'validation_split': 'validation', 'doc_to_text': 'Question: {{goal}}\nAnswer:', 'doc_to_target': 'label', 'doc_to_choice': '{{[sol1, sol2]}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'acc_norm', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'goal', 'metadata': {'version': 1.0}}, 'squadv2': {'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n\n'], 'do_sample': False}, 'repeats': 1, 'should_decontaminate': False, 'metadata': {'version': 3}}, 'truthfulqa_gen': {'task': 'truthfulqa_gen', 'group': ['truthfulqa'], 'dataset_path': 'truthful_qa', 'dataset_name': 'generation', 'validation_split': 'validation', 'process_docs': 'def process_docs_gen(dataset: datasets.Dataset) -> datasets.Dataset:\n    return dataset.map(preprocess_function)\n', 'doc_to_text': "{% set prompt_qa = 'Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.'%}{{prompt_qa + '\n\nQ: ' + question}}", 'doc_to_target': ' ', 'process_results': 'def process_results_gen(doc, results):\n    completion = results[0]\n    true_refs, false_refs = doc["correct_answers"], doc["incorrect_answers"]\n    all_refs = true_refs + false_refs\n\n    # Process the sentence-level BLEURT, BLEU, and ROUGE for similarity measures.\n\n    # # BLEURT\n    # bleurt_scores_true = self.bleurt.compute(\n    #     predictions=[completion] * len(true_refs), references=true_refs\n    # )["scores"]\n    # bleurt_scores_false = self.bleurt.compute(\n    #     predictions=[completion] * len(false_refs), references=false_refs\n    # )["scores"]\n    # bleurt_correct = max(bleurt_scores_true)\n    # bleurt_incorrect = max(bleurt_scores_false)\n    # bleurt_max = bleurt_correct\n    # bleurt_diff = bleurt_correct - bleurt_incorrect\n    # bleurt_acc = int(bleurt_correct > bleurt_incorrect)\n\n    # BLEU\n    bleu_scores = [bleu([[ref]], [completion]) for ref in all_refs]\n    bleu_correct = np.nanmax(bleu_scores[: len(true_refs)])\n    bleu_incorrect = np.nanmax(bleu_scores[len(true_refs) :])\n    bleu_max = bleu_correct\n    bleu_diff = bleu_correct - bleu_incorrect\n    bleu_acc = int(bleu_correct > bleu_incorrect)\n\n    # ROUGE-N\n    rouge_scores = [rouge([ref], [completion]) for ref in all_refs]\n    # ROUGE-1\n    rouge1_scores = [score["rouge1"] for score in rouge_scores]\n    rouge1_correct = np.nanmax(rouge1_scores[: len(true_refs)])\n    rouge1_incorrect = np.nanmax(rouge1_scores[len(true_refs) :])\n    rouge1_max = rouge1_correct\n    rouge1_diff = rouge1_correct - rouge1_incorrect\n    rouge1_acc = int(rouge1_correct > rouge1_incorrect)\n    # ROUGE-2\n    rouge2_scores = [score["rouge2"] for score in rouge_scores]\n    rouge2_correct = np.nanmax(rouge2_scores[: len(true_refs)])\n    rouge2_incorrect = np.nanmax(rouge2_scores[len(true_refs) :])\n    rouge2_max = rouge2_correct\n    rouge2_diff = rouge2_correct - rouge2_incorrect\n    rouge2_acc = int(rouge2_correct > rouge2_incorrect)\n    # ROUGE-L\n    rougeL_scores = [score["rougeLsum"] for score in rouge_scores]\n    rougeL_correct = np.nanmax(rougeL_scores[: len(true_refs)])\n    rougeL_incorrect = np.nanmax(rougeL_scores[len(true_refs) :])\n    rougeL_max = rougeL_correct\n    rougeL_diff = rougeL_correct - rougeL_incorrect\n    rougeL_acc = int(rougeL_correct > rougeL_incorrect)\n\n    return {\n        # "bleurt_max": bleurt_max,\n        # "bleurt_acc": bleurt_acc,\n        # "bleurt_diff": bleurt_diff,\n        "bleu_max": bleu_max,\n        "bleu_acc": bleu_acc,\n        "bleu_diff": bleu_diff,\n        "rouge1_max": rouge1_max,\n        "rouge1_acc": rouge1_acc,\n        "rouge1_diff": rouge1_diff,\n        "rouge2_max": rouge2_max,\n        "rouge2_acc": rouge2_acc,\n        "rouge2_diff": rouge2_diff,\n        "rougeL_max": rougeL_max,\n        "rougeL_acc": rougeL_acc,\n        "rougeL_diff": rougeL_diff,\n    }\n', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'bleu_max', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'bleu_acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'bleu_diff', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rouge1_max', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rouge1_acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rouge1_diff', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rouge2_max', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rouge2_acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rouge2_diff', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rougeL_max', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rougeL_acc', 'aggregation': 'mean', 'higher_is_better': True}, {'metric': 'rougeL_diff', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'generate_until', 'generation_kwargs': {'until': ['\n\n'], 'do_sample': False}, 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'question', 'metadata': {'version': 3.0}}, 'truthfulqa_mc1': {'task': 'truthfulqa_mc1', 'group': ['truthfulqa'], 'dataset_path': 'truthful_qa', 'dataset_name': 'multiple_choice', 'validation_split': 'validation', 'doc_to_text': "{% set prompt_qa = 'Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.'%}{{prompt_qa + '\n\nQ: ' + question + '\nA:'}}", 'doc_to_target': 0, 'doc_to_choice': '{{mc1_targets.choices}}', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'question', 'metadata': {'version': 2.0}}, 'truthfulqa_mc2': {'task': 'truthfulqa_mc2', 'group': ['truthfulqa'], 'dataset_path': 'truthful_qa', 'dataset_name': 'multiple_choice', 'validation_split': 'validation', 'doc_to_text': "{% set prompt_qa = 'Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.'%}{{prompt_qa + '\n\nQ: ' + question + '\nA:'}}", 'doc_to_target': 0, 'doc_to_choice': '{{mc2_targets.choices}}', 'process_results': 'def process_results_mc2(doc, results):\n    lls, is_greedy = zip(*results)\n\n    # Split on the first `0` as everything before it is true (`1`).\n    split_idx = list(doc["mc2_targets"]["labels"]).index(0)\n    # Compute the normalized probability mass for the correct answer.\n    ll_true, ll_false = lls[:split_idx], lls[split_idx:]\n    p_true, p_false = np.exp(np.array(ll_true)), np.exp(np.array(ll_false))\n    p_true = p_true / (sum(p_true) + sum(p_false))\n\n    return {"acc": sum(p_true)}\n', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'question', 'metadata': {'version': 2.0}}, 'winogrande': {'task': 'winogrande', 'dataset_path': 'winogrande', 'dataset_name': 'winogrande_xl', 'training_split': 'train', 'validation_split': 'validation', 'doc_to_text': 'def doc_to_text(doc):\n    answer_to_num = {"1": 0, "2": 1}\n    return answer_to_num[doc["answer"]]\n', 'doc_to_target': 'def doc_to_target(doc):\n    idx = doc["sentence"].index("_") + 1\n    return doc["sentence"][idx:].strip()\n', 'doc_to_choice': 'def doc_to_choice(doc):\n    idx = doc["sentence"].index("_")\n    options = [doc["option1"], doc["option2"]]\n    return [doc["sentence"][:idx] + opt for opt in options]\n', 'description': '', 'target_delimiter': ' ', 'fewshot_delimiter': '\n\n', 'num_fewshot': 0, 'metric_list': [{'metric': 'acc', 'aggregation': 'mean', 'higher_is_better': True}], 'output_type': 'multiple_choice', 'repeats': 1, 'should_decontaminate': True, 'doc_to_decontamination_query': 'sentence', 'metadata': {'version': 1.0}}}, 'cli_configs': {'model': 'hf', 'model_args': 'pretrained=microsoft/phi-1_5,cache_dir=/scratch0/aszablew/raw_models_cache,trust_remote_code=True', 'batch_size': 'auto', 'batch_sizes': [64], 'device': 'cuda', 'use_cache': '/scratch0/aszablew/phi-1_5-eval1/.cache/phi-1_5/cache', 'limit': None, 'bootstrap_iters': 100000, 'gen_kwargs': None, 'random_seed': 0, 'numpy_seed': 1234, 'torch_seed': 1234, 'fewshot_seed': 1234}}
2024-06-09 13:46:23,559 INFO    MainThread:45321 [wandb_run.py:_finish():2109] finishing run snlp/jmhotau7
2024-06-09 13:46:23,560 INFO    MainThread:45321 [wandb_run.py:_atexit_cleanup():2349] got exitcode: 0
2024-06-09 13:46:23,560 INFO    MainThread:45321 [wandb_run.py:_restore():2332] restore
2024-06-09 13:46:23,560 INFO    MainThread:45321 [wandb_run.py:_restore():2338] restore done
2024-06-09 13:46:24,977 INFO    MainThread:45321 [wandb_run.py:_footer_history_summary_info():4008] rendering history
2024-06-09 13:46:24,980 INFO    MainThread:45321 [wandb_run.py:_footer_history_summary_info():4040] rendering summary
