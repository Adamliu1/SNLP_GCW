{"columns": ["Tasks", "Version", "Filter", "num_fewshot", "Metric", "Value", "Stderr"], "data": [["winogrande", 1.0, "none", null, "acc", "0.580110497237569", "0.0139"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_max", "17.560549967493852", "0.6295"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_acc", "0.3818849449204406", "0.0170"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_diff", "-4.555746488904364", "0.6487"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_max", "37.884934898111254", "0.8877"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_acc", "0.28518971848225216", "0.0158"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_diff", "-9.28366883685431", "0.8528"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_max", "20.800716656025962", "0.9009"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_acc", "0.1762545899632803", "0.0133"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_diff", "-9.814472454050547", "0.9054"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_max", "35.33095132524075", "0.8686"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_acc", "0.26560587515299877", "0.0155"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_diff", "-9.110677458856951", "0.8434"], ["truthfulqa_mc1", 2.0, "none", 0, "acc", "0.2521419828641371", "0.0152"], ["truthfulqa_mc2", 2.0, "none", 0, "acc", "0.4036777437130198", "0.0141"], ["toxigen", 1.0, "none", null, "acc", "0.5585106382978723", "0.0162"], ["toxigen", 1.0, "none", null, "acc_norm", "0.4308510638297872", "0.0162"], ["mmlu_formal_logic", 0.0, "none", null, "acc", "0.1984126984126984", "0.0357"], ["mmlu_high_school_european_history", 0.0, "none", null, "acc", "0.23030303030303031", "0.0329"], ["mmlu_high_school_us_history", 0.0, "none", null, "acc", "0.24509803921568626", "0.0302"], ["mmlu_high_school_world_history", 0.0, "none", null, "acc", "0.21518987341772153", "0.0268"], ["mmlu_international_law", 0.0, "none", null, "acc", "0.35537190082644626", "0.0437"], ["mmlu_jurisprudence", 0.0, "none", null, "acc", "0.28703703703703703", "0.0437"], ["mmlu_logical_fallacies", 0.0, "none", null, "acc", "0.3006134969325153", "0.0360"], ["mmlu_moral_disputes", 0.0, "none", null, "acc", "0.2745664739884393", "0.0240"], ["mmlu_moral_scenarios", 0.0, "none", null, "acc", "0.24804469273743016", "0.0144"], ["mmlu_philosophy", 0.0, "none", null, "acc", "0.24758842443729903", "0.0245"], ["mmlu_prehistory", 0.0, "none", null, "acc", "0.29012345679012347", "0.0253"], ["mmlu_professional_law", 0.0, "none", null, "acc", "0.26010430247718386", "0.0112"], ["mmlu_world_religions", 0.0, "none", null, "acc", "0.24561403508771928", "0.0330"], ["mmlu_business_ethics", 0.0, "none", null, "acc", "0.24", "0.0429"], ["mmlu_clinical_knowledge", 0.0, "none", null, "acc", "0.24528301886792453", "0.0265"], ["mmlu_college_medicine", 0.0, "none", null, "acc", "0.26011560693641617", "0.0335"], ["mmlu_global_facts", 0.0, "none", null, "acc", "0.28", "0.0451"], ["mmlu_human_aging", 0.0, "none", null, "acc", "0.28699551569506726", "0.0304"], ["mmlu_management", 0.0, "none", null, "acc", "0.18446601941747573", "0.0384"], ["mmlu_marketing", 0.0, "none", null, "acc", "0.25213675213675213", "0.0284"], ["mmlu_medical_genetics", 0.0, "none", null, "acc", "0.24", "0.0429"], ["mmlu_miscellaneous", 0.0, "none", null, "acc", "0.25925925925925924", "0.0157"], ["mmlu_nutrition", 0.0, "none", null, "acc", "0.24509803921568626", "0.0246"], ["mmlu_professional_accounting", 0.0, "none", null, "acc", "0.2801418439716312", "0.0268"], ["mmlu_professional_medicine", 0.0, "none", null, "acc", "0.15808823529411764", "0.0222"], ["mmlu_virology", 0.0, "none", null, "acc", "0.24096385542168675", "0.0333"], ["mmlu_econometrics", 0.0, "none", null, "acc", "0.2543859649122807", "0.0410"], ["mmlu_high_school_geography", 0.0, "none", null, "acc", "0.2474747474747475", "0.0307"], ["mmlu_high_school_government_and_politics", 0.0, "none", null, "acc", "0.2538860103626943", "0.0314"], ["mmlu_high_school_macroeconomics", 0.0, "none", null, "acc", "0.2128205128205128", "0.0208"], ["mmlu_high_school_microeconomics", 0.0, "none", null, "acc", "0.20168067226890757", "0.0261"], ["mmlu_high_school_psychology", 0.0, "none", null, "acc", "0.22935779816513763", "0.0180"], ["mmlu_human_sexuality", 0.0, "none", null, "acc", "0.183206106870229", "0.0339"], ["mmlu_professional_psychology", 0.0, "none", null, "acc", "0.2630718954248366", "0.0178"], ["mmlu_public_relations", 0.0, "none", null, "acc", "0.20909090909090908", "0.0390"], ["mmlu_security_studies", 0.0, "none", null, "acc", "0.2163265306122449", "0.0264"], ["mmlu_sociology", 0.0, "none", null, "acc", "0.22885572139303484", "0.0297"], ["mmlu_us_foreign_policy", 0.0, "none", null, "acc", "0.22", "0.0416"], ["mmlu_abstract_algebra", 0.0, "none", null, "acc", "0.27", "0.0446"], ["mmlu_anatomy", 0.0, "none", null, "acc", "0.3333333333333333", "0.0407"], ["mmlu_astronomy", 0.0, "none", null, "acc", "0.27631578947368424", "0.0364"], ["mmlu_college_biology", 0.0, "none", null, "acc", "0.24305555555555555", "0.0359"], ["mmlu_college_chemistry", 0.0, "none", null, "acc", "0.18", "0.0386"], ["mmlu_college_computer_science", 0.0, "none", null, "acc", "0.25", "0.0435"], ["mmlu_college_mathematics", 0.0, "none", null, "acc", "0.21", "0.0409"], ["mmlu_college_physics", 0.0, "none", null, "acc", "0.22549019607843138", "0.0416"], ["mmlu_computer_security", 0.0, "none", null, "acc", "0.24", "0.0429"], ["mmlu_conceptual_physics", 0.0, "none", null, "acc", "0.2", "0.0261"], ["mmlu_electrical_engineering", 0.0, "none", null, "acc", "0.27586206896551724", "0.0372"], ["mmlu_elementary_mathematics", 0.0, "none", null, "acc", "0.2724867724867725", "0.0229"], ["mmlu_high_school_biology", 0.0, "none", null, "acc", "0.25161290322580643", "0.0247"], ["mmlu_high_school_chemistry", 0.0, "none", null, "acc", "0.2857142857142857", "0.0318"], ["mmlu_high_school_computer_science", 0.0, "none", null, "acc", "0.33", "0.0473"], ["mmlu_high_school_mathematics", 0.0, "none", null, "acc", "0.26666666666666666", "0.0270"], ["mmlu_high_school_physics", 0.0, "none", null, "acc", "0.2582781456953642", "0.0357"], ["mmlu_high_school_statistics", 0.0, "none", null, "acc", "0.2222222222222222", "0.0284"], ["mmlu_machine_learning", 0.0, "none", null, "acc", "0.25", "0.0411"], ["hellaswag", 1.0, "none", null, "acc", "0.3990240987851026", "0.0049"], ["hellaswag", 1.0, "none", null, "acc_norm", "0.5212109141605258", "0.0050"], ["gsm8k", 3.0, "strict-match", 5, "exact_match", "0.009855951478392721", "0.0027"], ["gsm8k", 3.0, "flexible-extract", 5, "exact_match", "0.015163002274450341", "0.0034"], ["arc_challenge", 1.0, "none", null, "acc", "0.2235494880546075", "0.0122"], ["arc_challenge", 1.0, "none", null, "acc_norm", "0.27303754266211605", "0.0130"]]}