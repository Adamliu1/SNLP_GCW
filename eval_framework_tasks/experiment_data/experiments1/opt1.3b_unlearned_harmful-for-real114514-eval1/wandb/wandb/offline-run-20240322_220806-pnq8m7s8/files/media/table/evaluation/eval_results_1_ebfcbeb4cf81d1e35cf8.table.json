{"columns": ["Tasks", "Version", "Filter", "num_fewshot", "Metric", "Value", "Stderr"], "data": [["winogrande", 1.0, "none", null, "acc", "0.5769534333070244", "0.0139"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_max", "13.66079371768979", "0.5361"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_acc", "0.3733170134638923", "0.0169"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_diff", "-2.4927823087135788", "0.5500"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_max", "29.145974902611325", "0.8908"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_acc", "0.23990208078335373", "0.0149"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_diff", "-7.711387656563164", "0.8218"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_max", "13.641730099316241", "0.7946"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_acc", "0.11627906976744186", "0.0112"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_diff", "-7.156416662446255", "0.7797"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_max", "27.175241641238266", "0.8554"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_acc", "0.2252141982864137", "0.0146"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_diff", "-7.46188114092931", "0.8168"], ["truthfulqa_mc1", 2.0, "none", 0, "acc", "0.26193390452876375", "0.0154"], ["truthfulqa_mc2", 2.0, "none", 0, "acc", "0.4277972193437038", "0.0143"], ["toxigen", 1.0, "none", null, "acc", "0.5457446808510639", "0.0162"], ["toxigen", 1.0, "none", null, "acc_norm", "0.4319148936170213", "0.0162"], ["mmlu_formal_logic", 0.0, "none", null, "acc", "0.25396825396825395", "0.0389"], ["mmlu_high_school_european_history", 0.0, "none", null, "acc", "0.2727272727272727", "0.0348"], ["mmlu_high_school_us_history", 0.0, "none", null, "acc", "0.2549019607843137", "0.0306"], ["mmlu_high_school_world_history", 0.0, "none", null, "acc", "0.25738396624472576", "0.0285"], ["mmlu_international_law", 0.0, "none", null, "acc", "0.2727272727272727", "0.0407"], ["mmlu_jurisprudence", 0.0, "none", null, "acc", "0.2777777777777778", "0.0433"], ["mmlu_logical_fallacies", 0.0, "none", null, "acc", "0.2883435582822086", "0.0356"], ["mmlu_moral_disputes", 0.0, "none", null, "acc", "0.2630057803468208", "0.0237"], ["mmlu_moral_scenarios", 0.0, "none", null, "acc", "0.2424581005586592", "0.0143"], ["mmlu_philosophy", 0.0, "none", null, "acc", "0.21864951768488747", "0.0235"], ["mmlu_prehistory", 0.0, "none", null, "acc", "0.2623456790123457", "0.0245"], ["mmlu_professional_law", 0.0, "none", null, "acc", "0.24185136897001303", "0.0109"], ["mmlu_world_religions", 0.0, "none", null, "acc", "0.29239766081871343", "0.0349"], ["mmlu_business_ethics", 0.0, "none", null, "acc", "0.3", "0.0461"], ["mmlu_clinical_knowledge", 0.0, "none", null, "acc", "0.24150943396226415", "0.0263"], ["mmlu_college_medicine", 0.0, "none", null, "acc", "0.19653179190751446", "0.0303"], ["mmlu_global_facts", 0.0, "none", null, "acc", "0.32", "0.0469"], ["mmlu_human_aging", 0.0, "none", null, "acc", "0.36771300448430494", "0.0324"], ["mmlu_management", 0.0, "none", null, "acc", "0.20388349514563106", "0.0399"], ["mmlu_marketing", 0.0, "none", null, "acc", "0.23504273504273504", "0.0278"], ["mmlu_medical_genetics", 0.0, "none", null, "acc", "0.24", "0.0429"], ["mmlu_miscellaneous", 0.0, "none", null, "acc", "0.24521072796934865", "0.0154"], ["mmlu_nutrition", 0.0, "none", null, "acc", "0.2222222222222222", "0.0238"], ["mmlu_professional_accounting", 0.0, "none", null, "acc", "0.2695035460992908", "0.0265"], ["mmlu_professional_medicine", 0.0, "none", null, "acc", "0.21323529411764705", "0.0249"], ["mmlu_virology", 0.0, "none", null, "acc", "0.2710843373493976", "0.0346"], ["mmlu_econometrics", 0.0, "none", null, "acc", "0.2631578947368421", "0.0414"], ["mmlu_high_school_geography", 0.0, "none", null, "acc", "0.2474747474747475", "0.0307"], ["mmlu_high_school_government_and_politics", 0.0, "none", null, "acc", "0.22279792746113988", "0.0300"], ["mmlu_high_school_macroeconomics", 0.0, "none", null, "acc", "0.2128205128205128", "0.0208"], ["mmlu_high_school_microeconomics", 0.0, "none", null, "acc", "0.24369747899159663", "0.0279"], ["mmlu_high_school_psychology", 0.0, "none", null, "acc", "0.24220183486238533", "0.0184"], ["mmlu_human_sexuality", 0.0, "none", null, "acc", "0.183206106870229", "0.0339"], ["mmlu_professional_psychology", 0.0, "none", null, "acc", "0.25163398692810457", "0.0176"], ["mmlu_public_relations", 0.0, "none", null, "acc", "0.24545454545454545", "0.0412"], ["mmlu_security_studies", 0.0, "none", null, "acc", "0.22040816326530613", "0.0265"], ["mmlu_sociology", 0.0, "none", null, "acc", "0.24378109452736318", "0.0304"], ["mmlu_us_foreign_policy", 0.0, "none", null, "acc", "0.2", "0.0402"], ["mmlu_abstract_algebra", 0.0, "none", null, "acc", "0.33", "0.0473"], ["mmlu_anatomy", 0.0, "none", null, "acc", "0.3333333333333333", "0.0407"], ["mmlu_astronomy", 0.0, "none", null, "acc", "0.24342105263157895", "0.0349"], ["mmlu_college_biology", 0.0, "none", null, "acc", "0.24305555555555555", "0.0359"], ["mmlu_college_chemistry", 0.0, "none", null, "acc", "0.11", "0.0314"], ["mmlu_college_computer_science", 0.0, "none", null, "acc", "0.28", "0.0451"], ["mmlu_college_mathematics", 0.0, "none", null, "acc", "0.24", "0.0429"], ["mmlu_college_physics", 0.0, "none", null, "acc", "0.2549019607843137", "0.0434"], ["mmlu_computer_security", 0.0, "none", null, "acc", "0.23", "0.0423"], ["mmlu_conceptual_physics", 0.0, "none", null, "acc", "0.2425531914893617", "0.0280"], ["mmlu_electrical_engineering", 0.0, "none", null, "acc", "0.2206896551724138", "0.0346"], ["mmlu_elementary_mathematics", 0.0, "none", null, "acc", "0.2671957671957672", "0.0228"], ["mmlu_high_school_biology", 0.0, "none", null, "acc", "0.25806451612903225", "0.0249"], ["mmlu_high_school_chemistry", 0.0, "none", null, "acc", "0.2955665024630542", "0.0321"], ["mmlu_high_school_computer_science", 0.0, "none", null, "acc", "0.3", "0.0461"], ["mmlu_high_school_mathematics", 0.0, "none", null, "acc", "0.26296296296296295", "0.0268"], ["mmlu_high_school_physics", 0.0, "none", null, "acc", "0.26490066225165565", "0.0360"], ["mmlu_high_school_statistics", 0.0, "none", null, "acc", "0.18981481481481483", "0.0267"], ["mmlu_machine_learning", 0.0, "none", null, "acc", "0.25", "0.0411"], ["hellaswag", 1.0, "none", null, "acc", "0.32214698267277436", "0.0047"], ["hellaswag", 1.0, "none", null, "acc_norm", "0.3685520812587134", "0.0048"], ["gsm8k", 3.0, "strict-match", 5, "exact_match", "0.0", "0.0000"], ["gsm8k", 3.0, "flexible-extract", 5, "exact_match", "0.003032600454890068", "0.0015"], ["arc_challenge", 1.0, "none", null, "acc", "0.2235494880546075", "0.0122"], ["arc_challenge", 1.0, "none", null, "acc_norm", "0.2773037542662116", "0.0131"]]}