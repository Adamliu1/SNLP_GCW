{"columns": ["Tasks", "Version", "Filter", "num_fewshot", "Metric", "Value", "Stderr"], "data": [["winogrande", 1.0, "none", null, "acc", "0.5824782951854776", "0.0139"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_max", "17.334164711659977", "0.6249"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_acc", "0.3818849449204406", "0.0170"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_diff", "-4.323094410706295", "0.6336"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_max", "37.45055385584579", "0.8832"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_acc", "0.2839657282741738", "0.0158"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_diff", "-9.248737216477837", "0.8548"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_max", "20.334386342077327", "0.8939"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_acc", "0.17258261933904528", "0.0132"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_diff", "-9.642394254113228", "0.8957"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_max", "34.87321535666695", "0.8635"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_acc", "0.2631578947368421", "0.0154"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_diff", "-8.920572636034446", "0.8351"], ["truthfulqa_mc1", 2.0, "none", 0, "acc", "0.24969400244798043", "0.0152"], ["truthfulqa_mc2", 2.0, "none", 0, "acc", "0.40419257013375387", "0.0141"], ["toxigen", 1.0, "none", null, "acc", "0.5553191489361702", "0.0162"], ["toxigen", 1.0, "none", null, "acc_norm", "0.4308510638297872", "0.0162"], ["mmlu_formal_logic", 0.0, "none", null, "acc", "0.20634920634920634", "0.0362"], ["mmlu_high_school_european_history", 0.0, "none", null, "acc", "0.23030303030303031", "0.0329"], ["mmlu_high_school_us_history", 0.0, "none", null, "acc", "0.24509803921568626", "0.0302"], ["mmlu_high_school_world_history", 0.0, "none", null, "acc", "0.21518987341772153", "0.0268"], ["mmlu_international_law", 0.0, "none", null, "acc", "0.35537190082644626", "0.0437"], ["mmlu_jurisprudence", 0.0, "none", null, "acc", "0.2962962962962963", "0.0441"], ["mmlu_logical_fallacies", 0.0, "none", null, "acc", "0.3006134969325153", "0.0360"], ["mmlu_moral_disputes", 0.0, "none", null, "acc", "0.2774566473988439", "0.0241"], ["mmlu_moral_scenarios", 0.0, "none", null, "acc", "0.24916201117318434", "0.0145"], ["mmlu_philosophy", 0.0, "none", null, "acc", "0.2540192926045016", "0.0247"], ["mmlu_prehistory", 0.0, "none", null, "acc", "0.29012345679012347", "0.0253"], ["mmlu_professional_law", 0.0, "none", null, "acc", "0.26010430247718386", "0.0112"], ["mmlu_world_religions", 0.0, "none", null, "acc", "0.24561403508771928", "0.0330"], ["mmlu_business_ethics", 0.0, "none", null, "acc", "0.24", "0.0429"], ["mmlu_clinical_knowledge", 0.0, "none", null, "acc", "0.24528301886792453", "0.0265"], ["mmlu_college_medicine", 0.0, "none", null, "acc", "0.26011560693641617", "0.0335"], ["mmlu_global_facts", 0.0, "none", null, "acc", "0.28", "0.0451"], ["mmlu_human_aging", 0.0, "none", null, "acc", "0.2825112107623318", "0.0302"], ["mmlu_management", 0.0, "none", null, "acc", "0.18446601941747573", "0.0384"], ["mmlu_marketing", 0.0, "none", null, "acc", "0.25213675213675213", "0.0284"], ["mmlu_medical_genetics", 0.0, "none", null, "acc", "0.26", "0.0441"], ["mmlu_miscellaneous", 0.0, "none", null, "acc", "0.25798212005108556", "0.0156"], ["mmlu_nutrition", 0.0, "none", null, "acc", "0.24183006535947713", "0.0245"], ["mmlu_professional_accounting", 0.0, "none", null, "acc", "0.2765957446808511", "0.0267"], ["mmlu_professional_medicine", 0.0, "none", null, "acc", "0.15441176470588236", "0.0220"], ["mmlu_virology", 0.0, "none", null, "acc", "0.24096385542168675", "0.0333"], ["mmlu_econometrics", 0.0, "none", null, "acc", "0.2543859649122807", "0.0410"], ["mmlu_high_school_geography", 0.0, "none", null, "acc", "0.2474747474747475", "0.0307"], ["mmlu_high_school_government_and_politics", 0.0, "none", null, "acc", "0.2538860103626943", "0.0314"], ["mmlu_high_school_macroeconomics", 0.0, "none", null, "acc", "0.21025641025641026", "0.0207"], ["mmlu_high_school_microeconomics", 0.0, "none", null, "acc", "0.20168067226890757", "0.0261"], ["mmlu_high_school_psychology", 0.0, "none", null, "acc", "0.23119266055045873", "0.0181"], ["mmlu_human_sexuality", 0.0, "none", null, "acc", "0.183206106870229", "0.0339"], ["mmlu_professional_psychology", 0.0, "none", null, "acc", "0.2679738562091503", "0.0179"], ["mmlu_public_relations", 0.0, "none", null, "acc", "0.20909090909090908", "0.0390"], ["mmlu_security_studies", 0.0, "none", null, "acc", "0.2163265306122449", "0.0264"], ["mmlu_sociology", 0.0, "none", null, "acc", "0.22388059701492538", "0.0295"], ["mmlu_us_foreign_policy", 0.0, "none", null, "acc", "0.22", "0.0416"], ["mmlu_abstract_algebra", 0.0, "none", null, "acc", "0.28", "0.0451"], ["mmlu_anatomy", 0.0, "none", null, "acc", "0.34074074074074073", "0.0409"], ["mmlu_astronomy", 0.0, "none", null, "acc", "0.27631578947368424", "0.0364"], ["mmlu_college_biology", 0.0, "none", null, "acc", "0.2361111111111111", "0.0355"], ["mmlu_college_chemistry", 0.0, "none", null, "acc", "0.18", "0.0386"], ["mmlu_college_computer_science", 0.0, "none", null, "acc", "0.25", "0.0435"], ["mmlu_college_mathematics", 0.0, "none", null, "acc", "0.21", "0.0409"], ["mmlu_college_physics", 0.0, "none", null, "acc", "0.22549019607843138", "0.0416"], ["mmlu_computer_security", 0.0, "none", null, "acc", "0.24", "0.0429"], ["mmlu_conceptual_physics", 0.0, "none", null, "acc", "0.19574468085106383", "0.0259"], ["mmlu_electrical_engineering", 0.0, "none", null, "acc", "0.27586206896551724", "0.0372"], ["mmlu_elementary_mathematics", 0.0, "none", null, "acc", "0.2724867724867725", "0.0229"], ["mmlu_high_school_biology", 0.0, "none", null, "acc", "0.25161290322580643", "0.0247"], ["mmlu_high_school_chemistry", 0.0, "none", null, "acc", "0.2857142857142857", "0.0318"], ["mmlu_high_school_computer_science", 0.0, "none", null, "acc", "0.34", "0.0476"], ["mmlu_high_school_mathematics", 0.0, "none", null, "acc", "0.27037037037037037", "0.0271"], ["mmlu_high_school_physics", 0.0, "none", null, "acc", "0.26490066225165565", "0.0360"], ["mmlu_high_school_statistics", 0.0, "none", null, "acc", "0.2175925925925926", "0.0281"], ["mmlu_machine_learning", 0.0, "none", null, "acc", "0.24107142857142858", "0.0406"], ["hellaswag", 1.0, "none", null, "acc", "0.39982075283808005", "0.0049"], ["hellaswag", 1.0, "none", null, "acc_norm", "0.5222067317267477", "0.0050"], ["gsm8k", 3.0, "strict-match", 5, "exact_match", "0.009855951478392721", "0.0027"], ["gsm8k", 3.0, "flexible-extract", 5, "exact_match", "0.015163002274450341", "0.0034"], ["arc_challenge", 1.0, "none", null, "acc", "0.2235494880546075", "0.0122"], ["arc_challenge", 1.0, "none", null, "acc_norm", "0.27559726962457337", "0.0131"]]}