{"columns": ["Tasks", "Version", "Filter", "num_fewshot", "Metric", "Value", "Stderr"], "data": [["winogrande", 1.0, "none", null, "acc", "0.5808997632202052", "0.0139"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_max", "18.107702997976105", "0.6450"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_acc", "0.37454100367197063", "0.0169"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_diff", "-4.373273469308519", "0.6487"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_max", "38.65168859747797", "0.8846"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_acc", "0.28518971848225216", "0.0158"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_diff", "-8.903854576741596", "0.8411"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_max", "21.802293190670174", "0.9123"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_acc", "0.18482252141982863", "0.0136"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_diff", "-9.359293862075193", "0.9031"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_max", "36.06462901223861", "0.8688"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_acc", "0.26560587515299877", "0.0155"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_diff", "-8.924277375146275", "0.8300"], ["truthfulqa_mc1", 2.0, "none", 0, "acc", "0.2594859241126071", "0.0153"], ["truthfulqa_mc2", 2.0, "none", 0, "acc", "0.40933031732666025", "0.0142"], ["toxigen", 1.0, "none", null, "acc", "0.55", "0.0162"], ["toxigen", 1.0, "none", null, "acc_norm", "0.4308510638297872", "0.0162"], ["mmlu_formal_logic", 0.0, "none", null, "acc", "0.20634920634920634", "0.0362"], ["mmlu_high_school_european_history", 0.0, "none", null, "acc", "0.24242424242424243", "0.0335"], ["mmlu_high_school_us_history", 0.0, "none", null, "acc", "0.25", "0.0304"], ["mmlu_high_school_world_history", 0.0, "none", null, "acc", "0.22362869198312235", "0.0271"], ["mmlu_international_law", 0.0, "none", null, "acc", "0.34710743801652894", "0.0435"], ["mmlu_jurisprudence", 0.0, "none", null, "acc", "0.28703703703703703", "0.0437"], ["mmlu_logical_fallacies", 0.0, "none", null, "acc", "0.294478527607362", "0.0358"], ["mmlu_moral_disputes", 0.0, "none", null, "acc", "0.2832369942196532", "0.0243"], ["mmlu_moral_scenarios", 0.0, "none", null, "acc", "0.23910614525139665", "0.0143"], ["mmlu_philosophy", 0.0, "none", null, "acc", "0.24437299035369775", "0.0244"], ["mmlu_prehistory", 0.0, "none", null, "acc", "0.30246913580246915", "0.0256"], ["mmlu_professional_law", 0.0, "none", null, "acc", "0.26010430247718386", "0.0112"], ["mmlu_world_religions", 0.0, "none", null, "acc", "0.2807017543859649", "0.0345"], ["mmlu_business_ethics", 0.0, "none", null, "acc", "0.23", "0.0423"], ["mmlu_clinical_knowledge", 0.0, "none", null, "acc", "0.23773584905660378", "0.0262"], ["mmlu_college_medicine", 0.0, "none", null, "acc", "0.26011560693641617", "0.0335"], ["mmlu_global_facts", 0.0, "none", null, "acc", "0.27", "0.0446"], ["mmlu_human_aging", 0.0, "none", null, "acc", "0.30493273542600896", "0.0309"], ["mmlu_management", 0.0, "none", null, "acc", "0.1941747572815534", "0.0392"], ["mmlu_marketing", 0.0, "none", null, "acc", "0.2692307692307692", "0.0291"], ["mmlu_medical_genetics", 0.0, "none", null, "acc", "0.25", "0.0435"], ["mmlu_miscellaneous", 0.0, "none", null, "acc", "0.2567049808429119", "0.0156"], ["mmlu_nutrition", 0.0, "none", null, "acc", "0.2222222222222222", "0.0238"], ["mmlu_professional_accounting", 0.0, "none", null, "acc", "0.2765957446808511", "0.0267"], ["mmlu_professional_medicine", 0.0, "none", null, "acc", "0.16544117647058823", "0.0226"], ["mmlu_virology", 0.0, "none", null, "acc", "0.21084337349397592", "0.0318"], ["mmlu_econometrics", 0.0, "none", null, "acc", "0.2631578947368421", "0.0414"], ["mmlu_high_school_geography", 0.0, "none", null, "acc", "0.23737373737373738", "0.0303"], ["mmlu_high_school_government_and_politics", 0.0, "none", null, "acc", "0.24870466321243523", "0.0312"], ["mmlu_high_school_macroeconomics", 0.0, "none", null, "acc", "0.21025641025641026", "0.0207"], ["mmlu_high_school_microeconomics", 0.0, "none", null, "acc", "0.21428571428571427", "0.0267"], ["mmlu_high_school_psychology", 0.0, "none", null, "acc", "0.23302752293577983", "0.0181"], ["mmlu_human_sexuality", 0.0, "none", null, "acc", "0.17557251908396945", "0.0334"], ["mmlu_professional_psychology", 0.0, "none", null, "acc", "0.2696078431372549", "0.0180"], ["mmlu_public_relations", 0.0, "none", null, "acc", "0.19090909090909092", "0.0376"], ["mmlu_security_studies", 0.0, "none", null, "acc", "0.20816326530612245", "0.0260"], ["mmlu_sociology", 0.0, "none", null, "acc", "0.22388059701492538", "0.0295"], ["mmlu_us_foreign_policy", 0.0, "none", null, "acc", "0.23", "0.0423"], ["mmlu_abstract_algebra", 0.0, "none", null, "acc", "0.29", "0.0456"], ["mmlu_anatomy", 0.0, "none", null, "acc", "0.31851851851851853", "0.0402"], ["mmlu_astronomy", 0.0, "none", null, "acc", "0.2565789473684211", "0.0355"], ["mmlu_college_biology", 0.0, "none", null, "acc", "0.25", "0.0362"], ["mmlu_college_chemistry", 0.0, "none", null, "acc", "0.19", "0.0394"], ["mmlu_college_computer_science", 0.0, "none", null, "acc", "0.25", "0.0435"], ["mmlu_college_mathematics", 0.0, "none", null, "acc", "0.23", "0.0423"], ["mmlu_college_physics", 0.0, "none", null, "acc", "0.21568627450980393", "0.0409"], ["mmlu_computer_security", 0.0, "none", null, "acc", "0.26", "0.0441"], ["mmlu_conceptual_physics", 0.0, "none", null, "acc", "0.2127659574468085", "0.0268"], ["mmlu_electrical_engineering", 0.0, "none", null, "acc", "0.3103448275862069", "0.0386"], ["mmlu_elementary_mathematics", 0.0, "none", null, "acc", "0.2619047619047619", "0.0226"], ["mmlu_high_school_biology", 0.0, "none", null, "acc", "0.25161290322580643", "0.0247"], ["mmlu_high_school_chemistry", 0.0, "none", null, "acc", "0.2955665024630542", "0.0321"], ["mmlu_high_school_computer_science", 0.0, "none", null, "acc", "0.34", "0.0476"], ["mmlu_high_school_mathematics", 0.0, "none", null, "acc", "0.25925925925925924", "0.0267"], ["mmlu_high_school_physics", 0.0, "none", null, "acc", "0.2582781456953642", "0.0357"], ["mmlu_high_school_statistics", 0.0, "none", null, "acc", "0.2222222222222222", "0.0284"], ["mmlu_machine_learning", 0.0, "none", null, "acc", "0.21428571428571427", "0.0389"], ["hellaswag", 1.0, "none", null, "acc", "0.39842660824536946", "0.0049"], ["hellaswag", 1.0, "none", null, "acc_norm", "0.520812587134037", "0.0050"], ["gsm8k", 3.0, "strict-match", 5, "exact_match", "0.01061410159211524", "0.0028"], ["gsm8k", 3.0, "flexible-extract", 5, "exact_match", "0.016679302501895376", "0.0035"], ["arc_challenge", 1.0, "none", null, "acc", "0.21245733788395904", "0.0120"], ["arc_challenge", 1.0, "none", null, "acc_norm", "0.26621160409556316", "0.0129"]]}