{"columns": ["Tasks", "Version", "Filter", "num_fewshot", "Metric", "Value", "Stderr"], "data": [["winogrande", 1.0, "none", null, "acc", "0.5824782951854776", "0.0139"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_max", "15.575033154513589", "0.5645"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_acc", "0.39657282741738065", "0.0171"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_diff", "-2.8991130309005504", "0.6206"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_max", "33.55393199427793", "0.9043"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_acc", "0.2766217870257038", "0.0157"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_diff", "-8.061232542632114", "0.8711"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_max", "16.96059026433792", "0.8559"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_acc", "0.16401468788249693", "0.0130"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_diff", "-7.538379452987342", "0.8702"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_max", "31.300079762404856", "0.8727"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_acc", "0.27050183598531213", "0.0156"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_diff", "-7.740054801548339", "0.8636"], ["truthfulqa_mc1", 2.0, "none", 0, "acc", "0.2582619339045288", "0.0153"], ["truthfulqa_mc2", 2.0, "none", 0, "acc", "0.42284753182163964", "0.0142"], ["toxigen", 1.0, "none", null, "acc", "0.5702127659574469", "0.0162"], ["toxigen", 1.0, "none", null, "acc_norm", "0.4340425531914894", "0.0162"], ["mmlu_formal_logic", 0.0, "none", null, "acc", "0.20634920634920634", "0.0362"], ["mmlu_high_school_european_history", 0.0, "none", null, "acc", "0.24242424242424243", "0.0335"], ["mmlu_high_school_us_history", 0.0, "none", null, "acc", "0.27450980392156865", "0.0313"], ["mmlu_high_school_world_history", 0.0, "none", null, "acc", "0.24472573839662448", "0.0280"], ["mmlu_international_law", 0.0, "none", null, "acc", "0.23140495867768596", "0.0385"], ["mmlu_jurisprudence", 0.0, "none", null, "acc", "0.3055555555555556", "0.0445"], ["mmlu_logical_fallacies", 0.0, "none", null, "acc", "0.27607361963190186", "0.0351"], ["mmlu_moral_disputes", 0.0, "none", null, "acc", "0.25722543352601157", "0.0235"], ["mmlu_moral_scenarios", 0.0, "none", null, "acc", "0.21899441340782122", "0.0138"], ["mmlu_philosophy", 0.0, "none", null, "acc", "0.17684887459807075", "0.0217"], ["mmlu_prehistory", 0.0, "none", null, "acc", "0.25925925925925924", "0.0244"], ["mmlu_professional_law", 0.0, "none", null, "acc", "0.24119947848761408", "0.0109"], ["mmlu_world_religions", 0.0, "none", null, "acc", "0.30409356725146197", "0.0353"], ["mmlu_business_ethics", 0.0, "none", null, "acc", "0.27", "0.0446"], ["mmlu_clinical_knowledge", 0.0, "none", null, "acc", "0.23773584905660378", "0.0262"], ["mmlu_college_medicine", 0.0, "none", null, "acc", "0.1907514450867052", "0.0300"], ["mmlu_global_facts", 0.0, "none", null, "acc", "0.3", "0.0461"], ["mmlu_human_aging", 0.0, "none", null, "acc", "0.35874439461883406", "0.0322"], ["mmlu_management", 0.0, "none", null, "acc", "0.1941747572815534", "0.0392"], ["mmlu_marketing", 0.0, "none", null, "acc", "0.21794871794871795", "0.0270"], ["mmlu_medical_genetics", 0.0, "none", null, "acc", "0.27", "0.0446"], ["mmlu_miscellaneous", 0.0, "none", null, "acc", "0.2554278416347382", "0.0156"], ["mmlu_nutrition", 0.0, "none", null, "acc", "0.2222222222222222", "0.0238"], ["mmlu_professional_accounting", 0.0, "none", null, "acc", "0.2695035460992908", "0.0265"], ["mmlu_professional_medicine", 0.0, "none", null, "acc", "0.2426470588235294", "0.0260"], ["mmlu_virology", 0.0, "none", null, "acc", "0.25903614457831325", "0.0341"], ["mmlu_econometrics", 0.0, "none", null, "acc", "0.22807017543859648", "0.0395"], ["mmlu_high_school_geography", 0.0, "none", null, "acc", "0.2222222222222222", "0.0296"], ["mmlu_high_school_government_and_politics", 0.0, "none", null, "acc", "0.22797927461139897", "0.0303"], ["mmlu_high_school_macroeconomics", 0.0, "none", null, "acc", "0.21794871794871795", "0.0209"], ["mmlu_high_school_microeconomics", 0.0, "none", null, "acc", "0.2184873949579832", "0.0268"], ["mmlu_high_school_psychology", 0.0, "none", null, "acc", "0.21284403669724772", "0.0175"], ["mmlu_human_sexuality", 0.0, "none", null, "acc", "0.19083969465648856", "0.0345"], ["mmlu_professional_psychology", 0.0, "none", null, "acc", "0.2549019607843137", "0.0176"], ["mmlu_public_relations", 0.0, "none", null, "acc", "0.17272727272727273", "0.0362"], ["mmlu_security_studies", 0.0, "none", null, "acc", "0.17959183673469387", "0.0246"], ["mmlu_sociology", 0.0, "none", null, "acc", "0.2537313432835821", "0.0308"], ["mmlu_us_foreign_policy", 0.0, "none", null, "acc", "0.21", "0.0409"], ["mmlu_abstract_algebra", 0.0, "none", null, "acc", "0.29", "0.0456"], ["mmlu_anatomy", 0.0, "none", null, "acc", "0.32592592592592595", "0.0405"], ["mmlu_astronomy", 0.0, "none", null, "acc", "0.21710526315789475", "0.0336"], ["mmlu_college_biology", 0.0, "none", null, "acc", "0.2638888888888889", "0.0369"], ["mmlu_college_chemistry", 0.0, "none", null, "acc", "0.1", "0.0302"], ["mmlu_college_computer_science", 0.0, "none", null, "acc", "0.27", "0.0446"], ["mmlu_college_mathematics", 0.0, "none", null, "acc", "0.22", "0.0416"], ["mmlu_college_physics", 0.0, "none", null, "acc", "0.19607843137254902", "0.0395"], ["mmlu_computer_security", 0.0, "none", null, "acc", "0.29", "0.0456"], ["mmlu_conceptual_physics", 0.0, "none", null, "acc", "0.28936170212765955", "0.0296"], ["mmlu_electrical_engineering", 0.0, "none", null, "acc", "0.3103448275862069", "0.0386"], ["mmlu_elementary_mathematics", 0.0, "none", null, "acc", "0.24074074074074073", "0.0220"], ["mmlu_high_school_biology", 0.0, "none", null, "acc", "0.25483870967741934", "0.0248"], ["mmlu_high_school_chemistry", 0.0, "none", null, "acc", "0.29064039408866993", "0.0319"], ["mmlu_high_school_computer_science", 0.0, "none", null, "acc", "0.32", "0.0469"], ["mmlu_high_school_mathematics", 0.0, "none", null, "acc", "0.23703703703703705", "0.0259"], ["mmlu_high_school_physics", 0.0, "none", null, "acc", "0.2582781456953642", "0.0357"], ["mmlu_high_school_statistics", 0.0, "none", null, "acc", "0.19444444444444445", "0.0270"], ["mmlu_machine_learning", 0.0, "none", null, "acc", "0.2767857142857143", "0.0425"], ["hellaswag", 1.0, "none", null, "acc", "0.3492332204740092", "0.0048"], ["hellaswag", 1.0, "none", null, "acc_norm", "0.425911173073093", "0.0049"], ["gsm8k", 3.0, "strict-match", 5, "exact_match", "0.0", "0.0000"], ["gsm8k", 3.0, "flexible-extract", 5, "exact_match", "0.01061410159211524", "0.0028"], ["arc_challenge", 1.0, "none", null, "acc", "0.2295221843003413", "0.0123"], ["arc_challenge", 1.0, "none", null, "acc_norm", "0.2815699658703072", "0.0131"]]}