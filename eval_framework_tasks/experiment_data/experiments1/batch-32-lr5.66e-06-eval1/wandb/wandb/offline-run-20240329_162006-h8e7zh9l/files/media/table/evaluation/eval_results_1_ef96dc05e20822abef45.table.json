{"columns": ["Tasks", "Version", "Filter", "num_fewshot", "Metric", "Value", "Stderr"], "data": [["winogrande", 1.0, "none", null, "acc", "0.5753749013417522", "0.0139"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_max", "14.250785407426493", "0.5262"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_acc", "0.4173806609547124", "0.0173"], ["truthfulqa_gen", 3.0, "none", 0, "bleu_diff", "-2.644459491454254", "0.5307"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_max", "30.83948127326374", "0.8315"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_acc", "0.27539779681762544", "0.0156"], ["truthfulqa_gen", 3.0, "none", 0, "rouge1_diff", "-9.32552005606113", "0.8068"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_max", "14.075472725583921", "0.7844"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_acc", "0.14320685434516525", "0.0123"], ["truthfulqa_gen", 3.0, "none", 0, "rouge2_diff", "-7.613591255607014", "0.7567"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_max", "28.723673218015335", "0.8045"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_acc", "0.27539779681762544", "0.0156"], ["truthfulqa_gen", 3.0, "none", 0, "rougeL_diff", "-8.808886946252679", "0.7875"], ["truthfulqa_mc1", 2.0, "none", 0, "acc", "0.2582619339045288", "0.0153"], ["truthfulqa_mc2", 2.0, "none", 0, "acc", "0.4158158583654908", "0.0142"], ["toxigen", 1.0, "none", null, "acc", "0.5595744680851064", "0.0162"], ["toxigen", 1.0, "none", null, "acc_norm", "0.42659574468085104", "0.0161"], ["mmlu_formal_logic", 0.0, "none", null, "acc", "0.1984126984126984", "0.0357"], ["mmlu_high_school_european_history", 0.0, "none", null, "acc", "0.296969696969697", "0.0357"], ["mmlu_high_school_us_history", 0.0, "none", null, "acc", "0.25", "0.0304"], ["mmlu_high_school_world_history", 0.0, "none", null, "acc", "0.2320675105485232", "0.0275"], ["mmlu_international_law", 0.0, "none", null, "acc", "0.3305785123966942", "0.0429"], ["mmlu_jurisprudence", 0.0, "none", null, "acc", "0.3055555555555556", "0.0445"], ["mmlu_logical_fallacies", 0.0, "none", null, "acc", "0.3128834355828221", "0.0364"], ["mmlu_moral_disputes", 0.0, "none", null, "acc", "0.2745664739884393", "0.0240"], ["mmlu_moral_scenarios", 0.0, "none", null, "acc", "0.24581005586592178", "0.0144"], ["mmlu_philosophy", 0.0, "none", null, "acc", "0.26366559485530544", "0.0250"], ["mmlu_prehistory", 0.0, "none", null, "acc", "0.2777777777777778", "0.0249"], ["mmlu_professional_law", 0.0, "none", null, "acc", "0.273142112125163", "0.0114"], ["mmlu_world_religions", 0.0, "none", null, "acc", "0.23976608187134502", "0.0327"], ["mmlu_business_ethics", 0.0, "none", null, "acc", "0.22", "0.0416"], ["mmlu_clinical_knowledge", 0.0, "none", null, "acc", "0.26037735849056604", "0.0270"], ["mmlu_college_medicine", 0.0, "none", null, "acc", "0.2658959537572254", "0.0337"], ["mmlu_global_facts", 0.0, "none", null, "acc", "0.21", "0.0409"], ["mmlu_human_aging", 0.0, "none", null, "acc", "0.19730941704035873", "0.0267"], ["mmlu_management", 0.0, "none", null, "acc", "0.20388349514563106", "0.0399"], ["mmlu_marketing", 0.0, "none", null, "acc", "0.23504273504273504", "0.0278"], ["mmlu_medical_genetics", 0.0, "none", null, "acc", "0.22", "0.0416"], ["mmlu_miscellaneous", 0.0, "none", null, "acc", "0.2413793103448276", "0.0153"], ["mmlu_nutrition", 0.0, "none", null, "acc", "0.26143790849673204", "0.0252"], ["mmlu_professional_accounting", 0.0, "none", null, "acc", "0.30851063829787234", "0.0276"], ["mmlu_professional_medicine", 0.0, "none", null, "acc", "0.21323529411764705", "0.0249"], ["mmlu_virology", 0.0, "none", null, "acc", "0.2289156626506024", "0.0327"], ["mmlu_econometrics", 0.0, "none", null, "acc", "0.2543859649122807", "0.0410"], ["mmlu_high_school_geography", 0.0, "none", null, "acc", "0.23232323232323232", "0.0301"], ["mmlu_high_school_government_and_politics", 0.0, "none", null, "acc", "0.25906735751295334", "0.0316"], ["mmlu_high_school_macroeconomics", 0.0, "none", null, "acc", "0.2128205128205128", "0.0208"], ["mmlu_high_school_microeconomics", 0.0, "none", null, "acc", "0.20168067226890757", "0.0261"], ["mmlu_high_school_psychology", 0.0, "none", null, "acc", "0.25504587155963304", "0.0187"], ["mmlu_human_sexuality", 0.0, "none", null, "acc", "0.21374045801526717", "0.0360"], ["mmlu_professional_psychology", 0.0, "none", null, "acc", "0.25326797385620914", "0.0176"], ["mmlu_public_relations", 0.0, "none", null, "acc", "0.15454545454545454", "0.0346"], ["mmlu_security_studies", 0.0, "none", null, "acc", "0.24081632653061225", "0.0274"], ["mmlu_sociology", 0.0, "none", null, "acc", "0.263681592039801", "0.0312"], ["mmlu_us_foreign_policy", 0.0, "none", null, "acc", "0.25", "0.0435"], ["mmlu_abstract_algebra", 0.0, "none", null, "acc", "0.3", "0.0461"], ["mmlu_anatomy", 0.0, "none", null, "acc", "0.32592592592592595", "0.0405"], ["mmlu_astronomy", 0.0, "none", null, "acc", "0.28289473684210525", "0.0367"], ["mmlu_college_biology", 0.0, "none", null, "acc", "0.24305555555555555", "0.0359"], ["mmlu_college_chemistry", 0.0, "none", null, "acc", "0.19", "0.0394"], ["mmlu_college_computer_science", 0.0, "none", null, "acc", "0.28", "0.0451"], ["mmlu_college_mathematics", 0.0, "none", null, "acc", "0.26", "0.0441"], ["mmlu_college_physics", 0.0, "none", null, "acc", "0.24509803921568626", "0.0428"], ["mmlu_computer_security", 0.0, "none", null, "acc", "0.28", "0.0451"], ["mmlu_conceptual_physics", 0.0, "none", null, "acc", "0.17872340425531916", "0.0250"], ["mmlu_electrical_engineering", 0.0, "none", null, "acc", "0.27586206896551724", "0.0372"], ["mmlu_elementary_mathematics", 0.0, "none", null, "acc", "0.25925925925925924", "0.0226"], ["mmlu_high_school_biology", 0.0, "none", null, "acc", "0.25806451612903225", "0.0249"], ["mmlu_high_school_chemistry", 0.0, "none", null, "acc", "0.3103448275862069", "0.0326"], ["mmlu_high_school_computer_science", 0.0, "none", null, "acc", "0.34", "0.0476"], ["mmlu_high_school_mathematics", 0.0, "none", null, "acc", "0.25925925925925924", "0.0267"], ["mmlu_high_school_physics", 0.0, "none", null, "acc", "0.25165562913907286", "0.0354"], ["mmlu_high_school_statistics", 0.0, "none", null, "acc", "0.26851851851851855", "0.0302"], ["mmlu_machine_learning", 0.0, "none", null, "acc", "0.26785714285714285", "0.0420"], ["hellaswag", 1.0, "none", null, "acc", "0.4039036048595897", "0.0049"], ["hellaswag", 1.0, "none", null, "acc_norm", "0.5238000398327026", "0.0050"], ["gsm8k", 3.0, "strict-match", 5, "exact_match", "0.014404852160727824", "0.0033"], ["gsm8k", 3.0, "flexible-extract", 5, "exact_match", "0.019711902956785442", "0.0038"], ["arc_challenge", 1.0, "none", null, "acc", "0.23037542662116042", "0.0123"], ["arc_challenge", 1.0, "none", null, "acc_norm", "0.2773037542662116", "0.0131"]]}